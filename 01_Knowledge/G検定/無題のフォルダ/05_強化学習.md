強化学習における<font color="#ffff00">割引率</font>は、将来得られる報酬を現在価値に換算するためのパラメータ（0から1の間で設定される）
# バンディットアルゴリズム
複数の選択肢の中から最適なものを効率的に見つけ出すための強化学習のアルゴリズム
<font color="#ffff00">探索</font>（新しい選択肢を試すこと）と<font color="#ffff00">活用</font>（最も良いとわかっている選択肢を選ぶこと）のバランスを最適化すること

---
# **ε-greedy方策**
バンディットアルゴリズムにおける探索と活用のバランスを取るためのシンプルな手法

活用・・・これまでの試行で最も良い結果を出した選択肢を選ぶ
探索・・・一定の確率 ε（イプシロン） で、ランダムに他の選択肢を選ぶ

εを大きくすると探索が増え、新しい選択肢を発見しやすくなりますが、最適な選択肢を引く機会を逃します。
εを小さくすると活用が増え、短期的な報酬は高まりますが、まだ見ぬ最適な選択肢を見落とす可能性があります。

---
# **マルコフ決定過程モデル（MDP）**
強化学習の多くの問題を数学的に定式化するためのフレームワーク

エージェントが累積報酬を最大化する最適な方策（各状態で行うべき行動の計画）をどう見つけるかを扱う
## **マルコフ性（Markov Property）**
「<font color="#ffff00">ある時点の未来の状態は、現在の状態にのみ依存し、それまでの過去の履歴には依存しない</font>」という性質

---
# 価値関数（Value Function）（Q値）
ある特定の状態や、ある状態である行動をとったときに、将来にわたってどれだけの累積報酬が期待できるかを表す関数
## 状態価値関数（V(s)）
ある状態sにいるときに、将来得られる累積報酬の期待値。
## 行動価値関数（Q(s,a)） 
ある状態sで特定の行動aをとったときに、将来得られる累積報酬の期待値。

多くのアルゴリズム（例: <font color="#ffff00">Q学習</font>）は、この価値関数を学習することで最適な行動を見つける

---
# **方策勾配（Policy Gradient）**
価値関数を学習するのではなく、累積報酬を最大化する最適な**方策**（行動方針）を直接学習する手法。
## 方策（Policy）
エージェントが特定の状態にいるときに、どの行動をどのくらいの確率でとるべきかを示す
### REINFORCE
方策勾配法（Policy Gradient）の最も基本的なアルゴリズム
エージェントが特定の行動をとった結果、得られた報酬の総和（累積報酬）に基づいて、その行動を**強化**（reinforce）
### Actor-Critic
REINFORCEの不安定性を改善するために考案された方策勾配法
#### Actor（アクター）
方策関数を学習し、どの状態ではどの行動をとるべきかを決定
#### Critic（クリティック）
クターがとった行動がどれだけ良かったかを評価