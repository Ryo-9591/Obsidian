# 深層強化学習（Deep Reinforcement Learning）
ニューラルネットワーク（深層学習）と強化学習を組み合わせた分野
複雑な環境での意思決定を学習するモデルを構築できる
## DQN (Deep Q-Network)
強化学習のQ学習に、深層学習を組み合わせた最初の成功例です。Q値をニューラルネットワークで近似し、 Atariゲームで人間を超える性能を示しました。
## ダブルDQN（Double DQN）
DQNの課題である<font color="#ffff00">Q値の過大評価を解決した手法</font>です。
学習用と評価用の2つのネットワークを使い分ける
→推定の安定性を向上
## デュエリングネットワーク（Dueling Network）
DQNのアーキテクチャを改良したもの
Q値を、状態の価値（V値）と各行動の優位性（A値）に<font color="#ffff00">分けて学習</font>することで、学習効率を高めた
## Rainbow
DQNの様々な改良手法（ダブルDQN、デュエリングネットワークなど）を組み合わせ、統合したモデル
単一のアルゴリズムで、各手法のメリットを享受できます。
## ノイジーネットワーク（Noisy Network）
ニューラルネットワークのパラメータに<font color="#ffff00">ガウス分布により発生させたノイズを重みとして計算に用いる</font>ことで、探索を促進する手法
→ε-greedy法のような単純な探索手法を必要としなくなる
## APE-X
DQNを分散学習に対応させたモデルです。複数のアクター（エージェント）が同時に環境と相互作用し、データを集めて一つのネットワークを学習させます。これにより、学習速度が劇的に向上しました。
## Agent57
各環境に適応する複数のアルゴリズムを動的に選択する強化学習エージェントです。Atari 57種類のゲームで最高の性能を達成しました。
# 深層生成モデル
深層ニューラルネットワーク（DNN）を活用して、訓練データの潜在的な構造や分布を捉え、新しいデータを生成するモデル
## GAN（Generative Adversarial Network）
GANは、生成器（Generator）と識別器（Discriminator）という2つのネットワークが互いに競い合いながら学習を進めるフレームワーク

・生成器: 偽のデータを生成し、識別器を騙そうとする
・識別器: 本物のデータと生成器が作った偽のデータを見分けようとする

この競争を通じて、生成器は次第に本物と見分けがつかないような高品質なデータを生成できるようになる
## VAE（Variational Autoencoder）
VAEは、エンコーダー（Encoder）とデコーダー（Decoder）から構成されるモデル

・エンコーダー
入力データを潜在空間と呼ばれる低次元のベクトル表現に変換
この潜在空間は、<font color="#ffff00">確率分布（通常は正規分布）として表現</font>される
・デコーダー: 潜在空間からサンプリングされたベクトルを受け取り、元のデータの形に復元（生成）

VAEは、GANに比べて生成されるデータの品質が劣ることがありますが、潜在空間が滑らかで、データの意味的な特徴をうまく捉えることができるという利点
## Diffusion Model
Diffusion Modelは、データをノイズから徐々に生成するモデル

・訓練
データに少しずつノイズを加えていき、最終的に完全にランダムな状態になるプロセスを学習
・生成
完全にランダムなノイズからスタートし、<font color="#ffff00">学習した逆のプロセス（ノイズを除去するプロセス）</font>を繰り返すことで、高品質な画像を生成