# AIの誕生

| 項目                                             | 説明                                                                       | 例                           |
| ---------------------------------------------- | ------------------------------------------------------------------------ | --------------------------- |
| レベル1<br><font color="#eeece1">ルールベースAI</font>  | すべての振る舞いがあらかじめ決められている<br>シンプルな制御システム                                     | エアコン、自動販売機                  |
| レベル2<br>古典的AI                                  | 探索・推論・知識データを用いて、<br>状況に応じた振る舞い                                           | 将棋やチェスのAI<br>エキスパートシステム     |
| レベル3<br><font color="#ffff00">機械学習AI</font>    | <font color="#eeece1">非常に多くのサンプルデータをもとに、<br>入力と出力の関係を学習するシステム</font><br> | レコメンデーションシステム<br>迷惑メールフィルター |
| レベル4<br><font color="#ffff00">ディープラーニング</font> | データから特徴量を自律的に学習する、<br>より高度なシステム                                          | 顔認証　音声認識　画像生成AI             |

| 年    | 内容                                                                                                                                                                                      |
| ---- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1946 | 世界初の汎用電子計算機<font color="#ffff00">エニアック(ENIAC) </font>誕生                                                                                                                                 |
| 1956 | ダートマス会議<br>①人工知能という言葉が<font color="#ffff00">ジョン・マッカーシー</font>によって初めて使用<br>②ハーバート・サイモン、アレン・ニューウェル、クリフ・ショーの数学の定理を自動的に証明する初期のAIプログラム<font color="#ffff00">ロジック・セオリスト</font> デモンストレーション<br> |

# 強いAIと弱いAIの提唱（ジョンサール）

| 項目   | 説明                    | 例        |
| ---- | --------------------- | -------- |
| 強いAI | 汎用的人工知能               | 自我を持ったAI |
| 弱いAI | 特化型人工知能<br>→中国語の部屋などで | AlphaGo  |

# 第一次AIブーム（1950年代後半〜1960年代）
<font color="#ffff00">トイ・プロブレム</font>（パズルや迷路探索といった限られた問題）で成果を上げる
現実世界の複雑な情報や未知の状況を扱うことができない<font color="#ffff00">フレーム問題</font>（1969）に直面

| 項目                                | 内容                                                                                                           |
| --------------------------------- | :----------------------------------------------------------------------------------------------------------- |
| ハノイの塔                             | ルール<br>1.  一度に一つの円盤しか動かせない。<br>2. 小さい円盤の上に大きい円盤を乗せてはいけない。<br>3. 3本ある杭のどれを使ってもよい。<br>最小手数で解くには、2n−1回の手数が必要となる |
| STRIPS                            | 前提・行動・結果を記述するプランニング                                                                                          |
| 1968～1970年<br>SHRDLU（テリー・ウィノグラード） | プランニングを「積み木の世界」で完全再現<br>自然言語だけで積み木を作れた                                                                       |
| 1984年<br>Cycプロジェクト（ダグ・レナート）       | 常識を記号の形で表現し、それを基にコンピュータが論理的な推論を行えるようにすることを目指す                                                                |


# ボードゲームパターン数

| ゲーム | パターン数  |
| --- | ------ |
| オセロ | 10^60  |
| チェス | 10^120 |
| 将棋  | 10^220 |
| 囲碁  | 10^360 |

# 探索アルゴリズム

|             |                                                                      | 図                                    |
| ----------- | -------------------------------------------------------------------- | ------------------------------------ |
| 幅優先探索       | 根から近いノードから順に探索<br>すべてのノードをキューに保持する必要がある<br>→メモリ不足になる                 | ![[Pasted image 20250901174929.png]] |
| 深さ優先探索      | 根からできるだけ深く探索<br>一度に一つの経路上のノードだけを記憶すればよい<br>→メモリ不足になりにくい<br>          | ![[Pasted image 20250901174941.png]] |
| Mini-Max法   | 相手も自分も最善を尽くすという前提で、ゲームのすべての可能な局面をツリー状に展開し、最終的な評価値が最大（ミニ）になるような手を選ぶ方法 | ![[Pasted image 20250901105639.png]] |
| Alpha-Beta法 | ミニマックス法を改良したもので、明らかに不利な局面の探索を途中で打ち切る                                 | ![[Pasted image 20250901105752.png]] |
| ヒューリスティック探索 | ヒューリスティクスを使って、より効率的に探索を進める方法                                         |                                      |
| モンテカルロ木探索   | ランダムな試行（シミュレーション）を繰り返して、最も有望な手を見つけ出します。                              |                                      |

## AI効果
かつて人工知能（AI）だと考えられていた技術が、普及して当たり前になると、もはやAIとはみなされなくなる現象
## 1950年：チューリングテスト（アラン・チューリング）
機械が人間と同等の知的振る舞いを示せるかを判定するためのテスト
## 1991年：ローブナーコンテスト（Loebner Prize） 
 チューリングテストの実践版として始まった年次コンテスト
## 1966年：イライザ（ELIZA） （ジョセフ・ワイゼンバウム） 人工無能
初期の対話プログラムです。キーワードに反応して質問を返すという単純な仕組み
イライザ効果：イライザを人間と勘違いする現象
## 身体性
知能獲得するには<font color="#ffff00">身体が不可欠</font>とする考え方
## 2045年：シンギュラリティ（レイ・カーツワイル）
AIが自らより優れたAIを開発し、そのAIがさらに優れたAIを開発するという「知能の爆発的連鎖」が起こることで、人類の知性を超える超知能（Superintelligence）が誕生するという考え

# 第二次AIブーム（1980年代〜）
知識獲得のボトルネック・シンボルグラウンディング問題
## エキスパートシステム
知識ベースと推論エンジンにより構成

| 名前      | 年    | 開発者            | 説明                                                                               |
| ------- | ---- | -------------- | -------------------------------------------------------------------------------- |
| DENDRAL | 1960 | エドワード・ファイゲンバウム | 質量スペクトルデータから、未知の有機化合物の分子構造を推定するシステム。専門家が利用する「ヒューリスティック（発見的手法）」をルール化して組み込んだ。      |
| MYCIN   | 1970 | スタンフォード大学      | 血液感染症の原因菌を特定し、治療薬を推奨するシステム。不確実な情報（発熱、症状など）から確率的に推論を行う「**確信度係数**」の概念を導入したことで知られる。 |
# オントロジー
意味ネットワークなどで用いられる知識の結び付け方の規則

| オントロジー種類       | 内容                                            |
| -------------- | --------------------------------------------- |
| ライトウェイト・オントロジー | 概念の分類（タクソノミー）や単純な語彙関係に焦点を当てた、比較的単純な構造のオントロジー。 |
| ヘビーウェイト・オントロジー | 複雑な概念間の関係性や制約、論理的な推論ルールを詳細に記述する、複雑な構造のオントロジー。 |

| 関連出来事      | 年    | 内容                                                                     |
| ---------- | ---- | ---------------------------------------------------------------------- |
| Cycプロジェクト  | 1984 | 人間のすべての一般常識をデータベース化し、知識ベースを構築することで人間と同等の推論システムを目指しました 。                |
| セマンティックウェブ |      | ウェブ上のデータをコンピュータが理解・解釈できるようにするための技術や概念の総称。オントロジーは、このセマンティックウェブの中核技術の一つ。 |
| ワトソン（IBM）  | 2011 | アメリカのクイズ番組ジョパディーで勝利(ライトオントロジーを使用)<br>質問に対して高速で検索しているだけ                 |
| 東ロボ君       | 2011 | 東京大学の合格を目指すも意味理解に苦しみ凍結                                                 |
# 第三次AIブーム（2000年代後半〜現在） 機械学習・特徴表現学習の時代

| 種類     | 内容            |
| ------ | ------------- |
| 教師あり学習 | ラベルを学習しラベルを予想 |
| 教師なし学習 | 構造やパターンを抽出    |
| 強化学習   | 将来の報酬を最大化     |
# 教師あり学習

## 回帰

| 項目          | 特徴                                                                             |
| ----------- | ------------------------------------------------------------------------------ |
| 線形回帰        | 入力特徴量と出力の間を、線形な関係でモデル化する最も基本的な手法。重み（係数）とバイアス（切片）を学習                            |
| 多項式回帰       | 線形回帰を拡張したもので、入力特徴量の多項式（2次、3次など）を使って非線形な関係をモデル化                                 |
| リッジ回帰       | 線形回帰に**L2正則化**（L2 regularization）を導入した手法。係数を小さくすることで、過学習を防ぎ、モデルの汎化能力を高める       |
| ラッソ回帰       | 線形回帰に**L1正則化**（L1 regularization）を導入した手法。重要でない特徴量の係数を強制的にゼロにすることで、特徴量選択を行う     |
| 決定木回帰       | データを階層的に分割し、各領域の平均値を出力として予測する手法。シンプルなルールベースのモデルを構築                             |
| ランダムフォレスト回帰 | 複数の決定木を組み合わせて予測を行う**アンサンブル学習**の一種。各決定木の予測の平均値を出力とする。                           |
| サポートベクター回帰  | サポートベクターマシン（SVM）を回帰に応用したもの。マージン内にできるだけ多くのデータポイントを収め、マージン外の誤差を最小化するようにモデルを学習する。 |


| 項目  | 特徴                                     |
| --- | -------------------------------------- |
| 単回帰 | 1つの独立変数（説明変数） を用いて、1つの従属変数（目的変数） を予測   |
| 重回帰 | 2つ以上の独立変数（説明変数） を用いて、1つの従属変数（目的変数） を予測 |

## 分類

| 項目          | 特徴                                                                                                                     |
| ----------- | ---------------------------------------------------------------------------------------------------------------------- |
| ロジスティック回帰   | 出力が0から1の間の確率であるため、分類問題によく用いられる。この確率をもとにクラスを判別する。                                                                       |
| サポートベクターマシン | マージン内にできるだけ多くのデータポイントを収め、マージン外の誤差を最小化するようにモデルを学習する。<br>**カーネルトリック**とそれを実現する**カーネル関数**によって、線形分離できない複雑なデータに対しても高い分類性能を発揮 |
| ランダムフォレスト   | 複数の決定木をランダムに作成し、それぞれの予測結果を多数決で最終的なクラスを決定する**アンサンブル学習**                                                                 |
| K-近傍法       | 予測したいデータに最も近い「K個」の訓練データを探し、それらのデータの多数決でクラスを決定する。                                                                       |
| ナイーブベイズ     | ベイズの定理に基づいて、各特徴量が独立であると仮定して確率を計算し、最も確率の高いクラスを予測する。                                                                     |
| ニューラルネットワーク | 脳の神経回路を模倣したモデル。入力層、中間層（隠れ層）、出力層から構成され、複雑な非線形関係を学習できる。                                                                  |

# アンサンブル学習

| 項目      | 特徴                                                                                           | 例                                     |
| ------- | -------------------------------------------------------------------------------------------- | ------------------------------------- |
| バギング    | 訓練データから**ブートストラップ法**（復元抽出）で複数のサブセットを作成し、それぞれでモデルを学習させる。最終的な予測は、**多数決（分類）** や**平均（回帰）** で決める。 | ランダムフォレスト                             |
| ブースティング | 性能の低いモデルを**逐次的**に学習させる。前のモデルが誤って予測したデータに重みを付けて、次のモデルがそのデータをより重視して学習する。                       | **AdaBoost**、**XGBoost**、**LightGBM** |
| スタッキング  | 複数の異なる種類のモデル（例：決定木、SVM）を学習させ、それらの予測結果を新たな特徴量として、**メタモデル**（最終的なモデル）を学習させる。                    |                                       |

| 項目       | 内容                                                                    | 特徴                                               |
| -------- | --------------------------------------------------------------------- | ------------------------------------------------ |
| AdaBoost | 誤分類されたデータに重みを加算し、次の学習器がそのデータをより重視して学習する。                              | 比較的速いが、XGBoostやLightGBMには劣る。                     |
| XGBoost  | **勾配ブースティング**の一種。損失関数の勾配を使ってモデルを逐次的に最適化する。過学習を防ぐための正則化機能を持つ。          | 並列処理に対応しているため、AdaBoostより速い。                      |
| LightGBM | **勾配ブースティング**の一種。XGBoostよりも高速で、大規模データに強い。葉っぱ単位で成長する**リーフワイズ**な決定木を使う。 | 非常に高速。**ヒストグラムベース**のアプローチにより、XGBoostより数倍速いことがある。 |

# 教師なし学習

## クラスタリング　データをグループ分けすること

| 項目          | 内容                                                                    |
| ----------- | --------------------------------------------------------------------- |
| 階層なしクラスタリング | クラスター数が固定されており、階層的な関係は作られません。アルゴリズムが高速で、大規模データに適しています。                |
| 階層ありクラスタリング | データポイント間の距離に基づいて、ツリー構造（デンドログラム）を構築します。クラスター数は後から決定できますが、計算コストが高くなります。 |

| 項目       | 内容                                                                                                   | 特徴                                                                  |
| -------- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| k-menas法 | **階層なしクラスタリング**の代表的な手法。事前に指定したクラスター数 (k) に基づき、データポイントを最も近いクラスター中心（重心）に割り当てていく。                       | **高速**でシンプル。大規模データに有効。クラスターの形状が**球状**であると仮定する。クラスター数を事前に決める必要がある。   |
| ウォード法    | **階層ありクラスタリング（凝集型）** の手法の一つ。クラスター間の距離を、**合併による情報損失**の増加量として定義する。クラスターの重心からの平方和を最小化するようにクラスターを併合していく。 | クラスターのサイズがほぼ均等になる傾向がある。**凝集型**の中で最も人気があり、バランスの取れたクラスターを形成する。        |
| 最短距離法    | **階層ありクラスタリング（凝集型）** の手法の一つ。2つのクラスター間の距離を、**それぞれのクラスターに属する最も近いデータポイント間の距離**と定義する。                    | **チェーン効果 (chaining effect)** が起こりやすく、細長いクラスターを形成しやすい。ノイズや外れ値に非常に敏感。 |
## 主成分分析（PCA）　データの次元を減らすこと

| 項目       | 内容                                                                        | 特徴                                                                                                                  |                                      |
| -------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------ |
| 特異値分解    | 行列を3つの行列（特異ベクトル、特異値）に分解する線形代数の手法。**主成分分析（PCA）** の実用的な計算方法として広く用いられる。      | - **線形**な手法。<br>- 元のデータの分散（バラつき）を最大化するように次元削減を行う。<br>- **情報圧縮**や**ノイズ除去**、**レコメンデーションシステム**に用いられる。                  |                                      |
| 多次元尺度構成法 | データ間の**距離や類似度**を保ちながら、高次元データを低次元空間にマッピングする手法。                             | データの**相対的な位置関係**を重視する。<br>- データそのものではなく、**データ間の距離行列**を入力として使用する。<br>- **心理学**や**マーケティング**分野で、消費者の好みなどを可視化するのに用いられる。 | ![[Pasted image 20250901142102.png]] |
| t-SNE    | 高次元空間でのデータ間の類似性を、低次元空間での確率分布として表現する**非線形**な次元削減手法。特に、**局所的な構造**を保つことに優れる。 | **非線形**な手法。<br>- クラスターやグループなどの**局所的な構造を非常に明確に可視化**できる。<br>- **データ可視化**に特化しており、機械学習の前処理としてはあまり使われない。                 | ![[Pasted image 20250901142317.png]] |

# 協調フィルタリング
多くのユーザーの行動や好みを利用して、新しいアイテムを推薦するレコメンデーション手法

| 項目               | 内容                                                         | 特徴                                                                           |
| ---------------- | ---------------------------------------------------------- | ---------------------------------------------------------------------------- |
| ユーザーベース協調フィルタリング | ターゲットユーザーと似た好みを持つ他のユーザーを見つけ出し、そのユーザーが良い評価をしたアイテムを推薦する手法です。 | 直感的で分かりやすい。ユーザー数が増加すると計算コストが急増する。新しいユーザーへの推薦が難しい「コールドスタート問題」に弱い。             |
| アイテムベース協調フィルタリング | ユーザーが過去に良い評価をしたアイテムと似たアイテムを推薦する手法です。アイテム間の類似度を計算する。        | ユーザーベースに比べて安定した推薦が可能。新しいアイテムが参入した際のコールドスタート問題に弱い。Netflixの推薦システムで使われたことで知られる。 |
| モデルベース協調フィルタリング  | データから潜在的なパターンや特徴を学習する機械学習モデルを構築し、それに基づいて推薦を行う手法です。         | スケーラビリティが高く、大規模データに対応できる。予測精度が高い。代表的な手法に行列分解がある。                             |
# トピックモデル
文書の集合（コーパス）の中から、潜在的な「トピック」を発見する教師なし学習の手法

| 項目          | 内容                                                                               |
| ----------- | -------------------------------------------------------------------------------- |
| 潜在的意味解析     | 文書と単語の共起行列に**特異値分解（SVD）** を適用することで、文書と単語を共通の潜在空間（トピック空間）に写像する手法です。               |
| 確率的潜在意味解析   | 文書がトピックの混合であり、各トピックが単語の確率分布であると仮定する**確率的モデル**です。文書と単語の共起を、トピックを介した確率としてモデル化します。  |
| 潜在的ディリクレ配分法 | **PLSA**を拡張した**ベイジアンモデル**です。各文書がトピックの確率分布の混合として生成され、各トピックが単語の確率分布として生成されると仮定します。 |
# 強化学習

| 項目           | 内容                                                                                                               |
| ------------ | ---------------------------------------------------------------------------------------------------------------- |
| バンディットアルゴリズム | 限られたリソースの中で、**探索（未知の選択肢を試す）** と**活用（既知の最良の選択肢を選ぶ）** のバランスを効率的にとるためのアルゴリズム                                        |
| ϵ-Greedy     | 確率εで**探索**を行い、確率1-εで**活用**を行うという、バンディット問題の解決策の一つ                                                                 |
| マルコフ決定過程     | 強化学習の多くの問題を数学的に定式化するためのフレームワーク<br><br>マルコフ性（Markov Property）<br>「ある時点の未来の状態は、現在の状態にのみ依存し、それまでの過去の履歴には依存しない」という性質 |


| 項目          | 内容                                                                                | 特徴と用途                                                                                                                                                                            |
| ----------- | --------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Q学習         | 価値ベース学習の代表。状態-行動ペアの**価値（Q値）** を学習し、そのQ値の最大化を目指す。行動選択には$\epsilon$-greedy法がよく用いられる。 | - オフポリシー（方策とは異なる行動から学習可能）。<br>- シンプルで実装が容易。<br>- 離散的な状態と行動空間を持つ問題に有効                                                                                                             |
| SARSA       | Q学習と同様の価値ベース学習。行動選択に用いた方策に基づいてQ値を更新する。                                            | - オンポリシー（行動に用いた方策から学習）。<br>- Q学習より安全な学習が期待される。                                                                                                                                   |
| DQN         | Q学習と**ディープラーニング**を組み合わせたもの。Q値をニューラルネットワークで近似することで、状態空間が膨大な問題にも対応可能。               | - **経験再生 (Experience Replay)**: 過去の経験をメモリに蓄え、そこからランダムにサンプリングして学習に用いる。これにより、データの相関が低減し、学習が安定する。 <br> - **ターゲットネットワーク**: 学習に用いるQ値の更新目標となるネットワークを、一定期間ごとに固定する。これにより、学習目標が安定し、発散を防ぐ。 |
| Double DQN  | **ターゲットQ値**の過大評価（overestimation）を抑制するために、Q値の計算方法を改良したDQN。                         | **Q値の過大評価を抑制**: 次の行動を評価する際に、オンラインネットワーク（最新のネットワーク）で行動を選び、ターゲットネットワークでその行動のQ値を評価する。これにより、過大評価が減り、安定した学習が可能になる。                                                                    |
| Dueling DQN | ネットワークのアーキテクチャを改良したDQN。                                                           | - **価値と優位性の分離**: ネットワークを「状態の価値 (V(s))」と「各行動の優位性 (Advantage, A(s,a))」に分離して学習する。これにより、各行動の価値をより正確に評価できる。                                                                           |
| Rainbow     | 上記の複数の改良点（DDQN, PER, Dueling DQN, C51など）を組み合わせたDQN。                               | - **高い性能**: それぞれの改良点の相乗効果により、単一のDQNよりも高い性能を発揮する。                                                                                                                                 |
| REINFORCE   | 方策勾配法（Policy Gradient Method）の代表。行動の価値ではなく、**行動方策（ポリシー）** を直接学習する。                | - 連続的な行動空間を持つ問題に適用可能。<br>- 方策ベース学習の最も基本的なアルゴリズム。                                                                                                                                 |
# モデルの評価

| 項目       | 内容                                                                                           | 特徴と用途                                                     |
| -------- | -------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| ホールドアウト法 | データを一度だけ訓練用とテスト用に分割し、テストデータでモデルを評価します。                                                       | シンプルで実装が容易。計算コストが低い。                                      |
| K-分割交差検証 | データをK個の等しいサイズのサブセットに分割します。そのうちの1つをテストデータ、残りのK−1個を訓練データとして学習と評価をK回繰り返します。最終的にK回の評価結果の平均を取ります。 | - データの利用効率が高い。<br>- 評価結果の信頼性が高い。<br>- ホールドアウト法より計算コストが高い。 |

# 混合行列
![[Pasted image 20250903234246.png]]
TP (True Positive): 実際に陽性で、陽性と予測
FP (False Positive): 実際に陰性で、陽性と予測
FN (False Negative): 実際に陽性で、陰性と予測
TN (True Negative): 実際に陰性で、陰性と予測

# パーセプトロン
複数の入力を受け取り、それらを重み付けして合計し、活性化関数を通して出力を生成する、神経細胞（ニューロン）を模倣したアルゴリズム

| 種類        | 説明                                                    | 主な特徴                                                                                              |
| --------- | ----------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| 単一パーセプトロン | 入力層と出力層のみを持つ最もシンプルなモデル。線形分離可能な問題のみを解決できる。             | - 線形分類器。<br>- 論理和（OR）、論理積（AND）は学習できるが、排他的論理和（XOR）は学習できない。<br>- 学習には、誤差を最小化するパーセプトロンの学習規則が用いられる。   |
| 多層パーセプトロン | 入力層と出力層の間に、1つまたは複数の**隠れ層**を持つモデル。各層は複数のパーセプトロンで構成される。 | - 非線形な問題を解決できる。<br>- 隠れ層が複雑な特徴を学習することで、より複雑な関係をモデル化できる。<br>- 学習には、誤差逆伝播法（Backpropagation） が用いられる。 |

# ディープラーニング
ニューラルネットワークの隠れ層を増やしたもの。誤差関数を最小化するアプローチ
# 誤差関数

| 種類         | 説明                                | 主な特徴                                                                                   |
| ---------- | --------------------------------- | -------------------------------------------------------------------------------------- |
| 平均二乗誤差     | 予測値と正解値の差の2乗を平均したもの。              | - 回帰問題で最も一般的に使われる。<br>- 誤差が大きいほど、より大きなペナルティを与える。<br>- 外れ値に敏感。                          |
| 平均絶対誤差     | 予測値と正解値の差の絶対値を平均したもの。             | - 回帰問題に用いられる。<br>- MSEより外れ値に強い。<br>- 勾配が一定であるため、学習の収束が遅くなることがある。                       |
| 交差エントロピー誤差 | モデルの出力する確率分布と、正解の真の確率分布との間の乖離を測る。 | - 分類問題で最も広く使われる。<br>- モデルが間違った予測をした際に、大きなペナルティを与える。<br>- 多クラス分類では、カテゴリカル交差エントロピーが使われる。 |
# 過学習への対策

| 手法名     | 説明                                                     | 特徴                                                   |
| ------- | ------------------------------------------------------ | ---------------------------------------------------- |
| 正則化     | モデルの複雑さにペナルティを与えることで、過剰な重み（係数）の学習を抑制する手法。              | L1正則化やL2正則化が代表的。重みを小さく保つことで、モデルの汎化能力を高める。            |
| ドロップアウト | 訓練時に、ランダムにニューロンを無効化する手法。これにより、モデルが特定のニューロンに依存しすぎるのを防ぐ。 | 一種のアンサンブル学習のような効果を持ち、汎化性能が向上する。テスト時にはすべてのニューロンを使用する。 |
| 早期終了    | 訓練誤差が減少し続け、検証誤差が最小値に達したところで訓練を打ち切る手法。                  | 過学習が始まる前に訓練を止めることができる。単純で効果的な方法。                     |
| データ拡張   | 既存の訓練データを変形（回転、反転、拡大・縮小など）させることで、擬似的にデータ量を増やす手法。       | 画像認識の分野で特に有効。モデルが多様なデータパターンを学習できるようになり、汎化性能が向上する。    |
| バッチ正規化  | 訓練のミニバッチごとに、データの平均を0、標準偏差を1に正規化する手法。                   | 内部共変量シフトを軽減し、学習を高速化・安定化させる。過学習の抑制にも効果がある。            |
# パラメータ最適化手法
損失関数を最小化するためにモデルのパラメータ（重みやバイアス）を更新するアルゴリズム

| アルゴリズム名               | 概要                                                                        | 主な特徴と利点                                                                          |
| --------------------- | ------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| 確率的勾配降下法 (SGD)オンライン学習 | 訓練データから**1つだけ**のサンプルをランダムに選び、その勾配を使ってパラメータを更新する。                          | - BGDより高速。<br>- 学習の軌跡がノイジーで、局所的最小値に陥りにくい。<br>- 収束が遅く、安定しないことがある。                 |
| バッチ下降法（最急下降法、バッチ学習）   | 全ての訓練データを使って勾配を計算し、パラメータを一度だけ更新する。                                        | - 非常に安定している。<br>- 計算コストが高く、大規模データには非効率的。<br>- 大域的最小値への収束が保証されている。                |
| ミニバッチ勾配下降法            | 訓練データから**ミニバッチ**と呼ばれるごく一部のサンプルを選び、その勾配を使ってパラメータを更新する。                     | - 収束の安定性と計算コストのバランスが最も良い。<br>- 現在、最も広く使われている手法。<br>- ハイパーパラメータとしてミニバッチのサイズを調整する。 |
| モーメンタム                | SGDに「慣性」の概念を導入した手法。過去の勾配の移動平均を考慮して、現在の勾配の方向を加速させる。                        | 谷のような平坦な場所で素早く収束する。振動を抑え、安定した学習を可能にする。                                           |
| AdaGrad               | 個々のパラメータごとに学習率を調整する手法。頻繁に出現するパラメータの学習率は小さく、あまり出現しないパラメータの学習率は大きく更新する。     | 疎なデータセット（まばらなデータ）に非常に有効。学習が進むにつれて学習率が減少し、勾配がゼロに近づきやすい。                           |
| Adadelta              | AdaGradの学習率が急激に減衰する問題を解決した手法。勾配の累積ではなく、過去の勾配の移動平均を用いる                     | AdaGradと異なり、**学習率の減衰が緩やか**。学習率のハイパーパラメータを必要としないのが大きな特徴。                          |
| RMSprop               | AdaGradの改良版として開発された手法。勾配の二乗の指数移動平均を用いて学習率を調整する。                           | AdaGradの欠点を克服し、**非凸最適化問題**にも適している。AdaDeltaと同様に、学習率の減衰が緩やか。                       |
| Adam                  | **RMSprop**と**モーメンタム**を組み合わせた手法。勾配の一次モーメント（平均）と二次モーメント（非中心分散）の指数移動平均を用いる。 | 非常に高性能で、**汎用性**が高い。多くのタスクでデフォルトのオプティマイザとして使われる。調整すべきハイパーパラメータが少ない。               |
| AdaBound・AMSBound     | Adamの欠点である、学習率が極端な値を取る可能性があることを改善した手法。学習率に下限と上限を設けて制御する。                  | 学習の初期段階ではAdaGradやAdamのように振る舞い、学習の後半ではSGDのように**安定**して収束する。                        |
# ハイパーパラメータの最適化

| 項目        | 説明                                                                                  | 主な特徴と利点                                                                                  |
| --------- | ----------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| グリッドサーチ   | 事前に定めたハイパーパラメータの組み合わせ（グリッド）を網羅的に試し、最も性能の良い組み合わせを見つける手法。                             | - シンプルで網羅的。最適な組み合わせを必ず見つけられる。<br>- パラメータの数が増えると、組み合わせが爆発的に増え、計算コストが非常に高くなる。              |
| ランダムサーチ   | 事前に定めた範囲からハイパーパラメータの組み合わせを**ランダムにサンプリング**して試し、最も性能の良い組み合わせを見つける手法。                  | - グリッドサーチより効率的。<br>- 重要なハイパーパラメータの値域をより広く探索できる。<br>- グリッドサーチと同じ計算コストでも、より良い結果を得られることが多い。 |
| ベイズ最適化    | 過去の試行結果を基に、次に試すべき最適なハイパーパラメータの組み合わせを予測する手法。獲得関数（Acquisition Function）を用いて、効率的に探索する。 | - 最も効率的な手法。計算コストが高いモデルの最適化に特に有効。<br>- 過去の情報を活用するため、試行回数を減らせる。<br>- 複雑な数理モデルに基づいている。      |
| 遺伝子アルゴリズム | 生物の進化（遺伝、突然変異、選択）を模倣した手法。性能の良いハイパーパラメータの組み合わせを「遺伝子」として扱い、世代を繰り返しながらより良い「遺伝子」を探索する。  | - 大域的最適解を見つけやすい。<br>- 直感的でわかりやすい。<br>- ベイズ最適化より効率は劣るが、多様な解を探索できる。                        |
| Optuna    | ベイズ最適化を応用したハイパーパラメータ最適化フレームワーク。試行結果から次に最適なパラメータの組み合わせを自動で探索する。                      | - 自動化されており、実装が容易。<br>- 並列化にも対応しており、高速。<br>- 様々なモデルやタスクに適用可能。                             |
# 誤差逆伝搬法の問題

| 問題点    | 説明                                                                                            | 対策                                                                                                                                        |
| ------ | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| 勾配消失問題 | 誤差が逆伝播する際に勾配がどんどん小さくなり、入力層に近い層の重みがほとんど更新されなくなる問題。特に、シグモイド関数やtanh関数などの活性化関数を持つ深いネットワークで発生しやすい。 | **ReLU（Rectified Linear Unit）** などの活性化関数を使用する。ReLUは、勾配が0になることがないため、勾配消失を防ぎやすい。また、**残差接続（Residual Connections）** を持つネットワーク構造（例：ResNet）も有効。 |
| 勾配爆発問題 | 誤差が逆伝播する際に勾配が非常に大きくなり、重みが極端に大きな値に更新される問題。これにより、学習が不安定になり、発散することがある。                           | **勾配クリッピング（Gradient Clipping）** を用いる。これは、勾配のノルム（大きさ）が一定の閾値を超えた場合に、その勾配をスケーリングして小さくする手法。                                                   |
# 活性化関数

| 種類                           | 式                                  | 特徴                                                                               | 主な用途                                                   |
| ---------------------------- | ---------------------------------- | -------------------------------------------------------------------------------- | ------------------------------------------------------ |
| シグモイド関数                      | f(x)=frac11+e−x                    | - 出力値が0から1の範囲に収まる。 <br> - 勾配が0に近づくため、勾配消失問題を引き起こしやすい。                            | - 出力層での二値分類の確率推定。 <br> - 歴史的に初期のニューラルネットワークで使われた。      |
| tanh関数                       | f(x)=fracex−e−xex+e−x              | - 出力値が-1から1の範囲に収まる。 <br> - シグモイド関数より勾配消失問題が緩和される。                                | - 中間層でよく使われる。 <br> - データの平均を0に近づける効果がある。               |
| ReLU (Rectified Linear Unit) | f(x)=max(0,x)<br><br>              | - 入力が0以下の場合は0、0より大きい場合は入力値そのものを返す。 <br> - 勾配消失問題が起こりにくい。 <br> - 計算が非常に高速。        | - 最も広く使われている中間層の活性化関数。 <br> - 多くの深層学習モデルでデフォルトとして使われる。 |
| Leaky ReLU                   | f(x)=max(alphax,x) （alphaは小さな正の定数） | - ReLUの改良版。入力が0以下でもわずかに勾配を持つ。 <br> - 「**死んだReLU**」（学習中に勾配が0になってしまうニューロン）問題を解決する。 | - ReLUの代替として、勾配消失を防ぎつつ安定した学習が必要な場合。                    |
| Softmax関数                    | f(x_i)=fracex_isum_jex_j           | - 入力された値の合計が1になるように、それぞれの値を確率として出力する。                                            | - 出力層での**多クラス分類**。 <br> - 各クラスの確率を表現するのに適している。         |
![[Pasted image 20250901170627.png]]
# CNN
ディープラーニングの中で最も成功しているモデル
# 物体認識・画像分類タスクに使われるCNN

| 項目           | 年代    | 人物                 | 説明                                                                                              | 図                                    |
| ------------ | ----- | ------------------ | ----------------------------------------------------------------------------------------------- | ------------------------------------ |
| ネオコグニトロン     | 1979年 | 福島邦彦               | 階層的な特徴抽出と位置ずれ不変性<br>S細胞：特徴を抽出する役割<br>C細胞：S細胞層で抽出された特徴の位置のずれを吸収                                  | ![[Pasted image 20250901190743.png]] |
| LeNet        | 1998年 | ヤン・ルカン             | 誤差逆伝搬法を用いるCNNの原型ともいえる初期のモデル。畳み込み層、プーリング層、全結合層で構成される基本的な構造を確立し、手書き数字認識に成功しました。                   | ![[Pasted image 20250901191054.png]] |
| AlexNet      | 2012年 | アレックス・クリジェフスキー     | 大規模なデータセット（ImageNet）で高い精度を達成し、CNNブームの火付け役となったモデル。GPUの活用、ReLU活性化関数、ドロップアウトを導入しました。<br><br>       | ![[Pasted image 20250901191937.png]] |
| GoogleNet    | 2014年 | Google             | **Inceptionモジュール**を導入し、複数のサイズの畳み込みフィルタを並列に適用する構造を特徴とします。これにより、計算コストを抑えつつ、モデルの幅を広げました。           | ![[Pasted image 20250901192258.png]] |
| VGG          | 2014年 | オックスフォード大学         | 3x3の小さなフィルタを複数積み重ねることで、表現力を高めたモデル。深いネットワークの構築における**シンプルな構造の重要性**を示しました。                         | ![[Pasted image 20250901192319.png]] |
| ResNet       | 2015年 | Microsoft Research | **残差接続（Residual Connection）** を導入し、深いネットワークでの勾配消失問題を解決しました。これにより、数百層の超深層ネットワークの学習が可能になりました。<br> | ![[Pasted image 20250901192330.png]] |
| Wide ResNet  | 2016年 | セルゲイ・ザハロフ          | ResNetの残差ブロックの幅（チャネル数）を広げることで、パラメータ数を大幅に増やさずに性能を向上                                              | ![[Pasted image 20250901192656.png]] |
| DenseNet     | 2017年 | Gao Huang          | 各層をそれ以前のすべての層に接続する**密な接続**を特徴とします。これにより、勾配消失問題をさらに緩和し、パラメータ数を削減しました。<br>                        | ![[Pasted image 20250901192727.png]] |
| SENET        | 2017年 | Momenta            | 畳み込み層の後に**Squeeze-and-Excitation (SE)ブロック**を追加したモデル。各チャネルの重要度を学習し、重要な特徴を強調する。<br><br>           | ![[Pasted image 20250901192834.png]] |
| MobileNet    | 2017年 | Google             | **深さ方向分離可能畳み込み（Depthwise Separable Convolution）** を導入し、計算コストを大幅に削減したモデル。                        | ![[Pasted image 20250901193514.png]] |
| NASNet       | 2018年 | Google             | **ニューラルアーキテクチャ探索（NAS）** を用いて、データセットで最適なネットワーク構造を自動で探索したモデル。                                     | ![[Pasted image 20250901193631.png]] |
| MnasNet      | 2018年 | Google             | **NAS**をモバイルデバイス向けに改良し、**精度**と**レイテンシ**（推論時間）の両方を最適化する目的関数を導入したモデル。                             |                                      |
| EfficientNet | 2019年 | Google             | モデルの**深さ、幅、解像度**の3つの次元をバランス良くスケーリングする新しいアプローチを提案したモデル。<br><br>                                  | ![[Pasted image 20250901193745.png]] |
# 物体検出タスクに使われるCNN
## 1段階検出器
物体の位置とクラスを単一のネットワークで同時に予測するモデル。高速な推論が可能

| モデル名                       | 年    | 主な特徴と貢献                                                                 |
| -------------------------- | ---- | ----------------------------------------------------------------------- |
| YOLO (You Only Look Once)  | 2016 | 物体検出タスクを回帰問題として捉え、単一のネットワークでバウンディングボックスとクラスを直接予測します。リアルタイムでの高速な検出が可能です。 |
| SSD (Single Shot Detector) | 2016 | 複数のスケール（解像度）の畳み込み層で特徴マップを抽出し、様々なサイズの物体を検出します。YOLOよりも精度が高く、速度も非常に速いです。   |
| RetinaNet                  | 2017 | 不均衡なデータ（前景と背景のクラス数の違い）を解決する**Focal Loss**を導入。高い精度を保ちつつ、高速な検出を実現しました。    |
## 2段階検出器
| モデル名                     | 年    | 主な特徴と貢献                                                                                          |
| ------------------------ | ---- | ------------------------------------------------------------------------------------------------ |
| R-CNN (Region-based CNN) | 2014 | 外部アルゴリズムで領域候補を抽出し、各領域をCNNで分類・回帰する初期のモデル。高い精度を達成しましたが、非常に低速でした。                                   |
| Fast R-CNN               | 2015 | R-CNNの改良版。**RoIプーリング層**を導入し、領域候補のCNN計算を一度で行うことで、速度を大幅に向上させました。                                   |
| Faster R-CNN             | 2015 | Fast R-CNNの改良版。外部アルゴリズムの代わりに、**領域提案ネットワーク（RPN）** を用いて領域候補をCNN内で生成。これにより、リアルタイムに近い速度で高い精度を達成しました。 |
# セグメンテーションタスクに使用されるCNN
## セグメンテーションタスクの種類

| タスク名        | 概要                                         | 目的と出力                                                               | 代表的なモデル                                                   |
| ----------- | ------------------------------------------ | ------------------------------------------------------------------- | --------------------------------------------------------- |
| セマンティック     | 画像内の全てのピクセルを<br>事前に定義されたカテゴリ（例: 車、道路、空）に分類 | 画像内の各ピクセルがどのカテゴリに属するかを識別<br>同じカテゴリの物体は区別しない                         | FCN (Fully Convolutional Network)<br>U-Net<br>DeepLabシリーズ |
| インスタンス      | 同じカテゴリに属する個々の物体を区別しながら、ピクセル単位で分類           | 同じカテゴリ内の各インスタンス（個体）を異なる色などで識別し、個々の物体の正確な輪郭を特定                       | Mask R-CNN                                                |
| パノプティック<br> | セマンティック＋インスタンス                             | 数えられる物体、例: 人、車と<br>数えられない領域、例: 空、道路<br>の両方を識別<br>画像全体を統一的にセグメンテーション | Panoptic FPN<br>UPSNet                                    |
![[Pasted image 20250901195854.png]]

| モデル名                              | 主な特徴と技術                                                                                                               |                                                              |
| --------------------------------- | --------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
| FCN (Fully Convolutional Network) | 従来のCNNの全結合層を、畳み込み層に置き換えることで、任意のサイズの入力画像に対応できるようにしたモデル。                                                                | セマンティックセグメンテーションの基本的なアーキテクチャを確立。ピクセルごとの分類が可能になりました。          |
| U-Net                             | FCNを改良したモデルで、**エンコーダ-デコーダ構造**に**スキップ接続**を追加。エンコーダで画像の特徴を圧縮し、デコーダで元の解像度に戻しながら、エンコーダからの詳細な特徴を結合します。<br><br><br><br><br> | 医療画像診断など、セグメンテーションの精度が非常に重要となる分野で広く使用されています。                 |
| PSPNet                            | Pyramid Scene Parsingという手法を導入<br>画像全体を複数の異なるスケールでプールし、その結果を統合することで、文脈情報を豊富に捉える                                        | これにより、シーン全体を理解し、画像内の物体だけでなく、<br>その背景や文脈も考慮した高精度なセグメンテーションを実現 |
| DeepLab                           | **アトラス畳み込み（Atrous Convolution）** や**アトラス空間ピラミッドプーリング（ASPP）** を用いて、より広範囲のコンテキスト情報を捉えるようにしたモデル。<br><br><br><br>         | 物体の輪郭をより正確に捉えることができ、セグメンテーションの精度が向上しました。                     |
| Mask R-CNN                        | 物体検出モデルの**Faster R-CNN**を拡張したモデル。物体検出と同時に、各物体に対するピクセル単位のマスクを生成します。                                                    | 物体検出とセグメンテーションを同時に実行できる。インスタンスセグメンテーションの代表的なモデルです。           |
| SegNet                            | セマンティックセグメンテーションのための深層学習モデルです。U-Netと同様に、エンコーダ-デコーダ構造を採用していますが、デコーダにおけるアップサンプリングの方法に独自の特徴があります。                        | 一般的なセマンティックセグメンテーション                                         |

## 姿勢推定タスクに使用されるCNN

| モデル名     | 登場年   | 主な特徴と技術登場年                                                                         |
| -------- | ----- | ---------------------------------------------------------------------------------- |
| OpenPose | 2017年 | 1枚の画像から、複数の人物の姿勢を同時に推定<br>関節点を示すヒートマップと、関節点間の接続を示す<br>部位接続（PAFs）を同時に予測する二つのブランチを持つ |
| HRNet    | 2019年 | ネットワーク全体を通して高解像度の特徴表現を維持<br>異なる解像度の並列ストリーム間で情報を繰り返し交換し、<br>高精度のキーポイントヒートマップを生成     |
## データ拡張
既存の訓練データを変形させることで、擬似的にデータ量を増やし、モデルの汎化性能を向上させる手法

| 手法名            | 説明                                                            |
| -------------- | ------------------------------------------------------------- |
| Cutout         | 訓練画像からランダムな領域を切り取り、その部分を0やランダムな値で埋める手法です。                     |
| Random Erasing | Cutoutと似ていますが、切り取る領域をランダムな矩形とし、その部分を画像の平均画素値などで埋めます。          |
| Mixup          | 2つの異なる画像を線形補間（ミックス）して新しい訓練画像を生成し、そのラベルも同じ比率で線形補間します。          |
| CutMix         | 訓練画像から切り取ったパッチを、別の訓練画像の同じ位置に貼り付ける手法です。ラベルは、パッチの割合に応じて線形補間します。 |

# RNN
隠れ層にリカレント結合を持ち、前の時刻の隠れ層の状態（情報）が、次の時刻の隠れ層の入力として渡される
## RNNが直前の隠れ層の状態しか記憶できないという課題を克服

| 項目          | 特徴                                       | 図                                    |
| ----------- | ---------------------------------------- | ------------------------------------ |
| エルマンネットワーク  | 直前の隠れ状態を次の層の入力に<br><br>直前の文脈を考慮するタスク     | ![[Pasted image 20250901202322.png]] |
| ジョルダンネットワーク | 直前の最終的な出力を次の層の入力に<br><br>過去の行動履歴を考慮するタスク | ![[Pasted image 20250901202339.png]] |

| モデル名                  | 年    | 主な特徴と貢献年                                                                                        |
| --------------------- | ---- | ----------------------------------------------------------------------------------------------- |
| RNN                   | 1980 | 隠れ層が自身の出力を次のステップの入力として受け取ることで、**過去の情報を記憶**する能力を持つ。これにより、時系列データや自然言語処理のタスクに適用可能になった。             |
| LSTM                  | 1997 | RNNの**長期依存性**の問題（勾配消失問題）を解決するために開発されたモデル。**入力ゲート、忘却ゲート、出力ゲート**という3つのゲートを使い、長期的な情報を効果的に記憶・忘却できる。 |
| Bi-RNN                | 1997 | 通常のRNNが過去の文脈しか考慮しないのに対し、Bi-RNNは**過去と未来の両方の文脈**を考慮して学習する。2つのRNNを並列に動かし、前向きと後ろ向きの情報を統合する。         |
| GRU                   | 2014 | LSTMの簡略化されたバージョン。**更新ゲート**と**リセットゲート**の2つのゲートのみを持ち、LSTMと同等の性能を保ちながら、計算コストが低い。                   |
| Encoder-Decoder Model | 2014 | 機械翻訳などのタスクで使われるモデル。**エンコーダ**が入力シーケンス全体を固定長のベクトルに圧縮し、**デコーダ**がそのベクトルから出力シーケンスを生成する。              |
| Transformer           | 2017 | RNNやLSTMの再帰的な構造を持たず、**Attentionメカニズム**のみでシーケンスデータの学習を行うモデル。並列処理が可能で、RNNモデルより遥かに高速               |

# エンコーダーデコーダー
シーケンス・トゥ・シーケンス（Seq2Seq）タスクに用いられるニューラルネットワークの一般的なアーキテクチャ

| モデル名                  | 構成要素                                                                                                                           | 主な特徴と用途                                                                                                   |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------- |
| RNNエンコーダ・デコーダ         | **エンコーダ**: RNN（RNN, LSTM, GRU）が入力シーケンス全体を読み込み、最後の隠れ状態を**コンテキストベクトル**として出力。<br>**デコーダ**: RNNがコンテキストベクトルを受け取り、出力シーケンスをステップごとに生成。 | - **機械翻訳**や**要約**の初期モデル。<br>- 長いシーケンスを1つの固定長ベクトルに圧縮するため、**情報損失**が生じやすい。                                   |
| Attention付きエンコーダ・デコーダ | RNNエンコーダ・デコーダに**アテンション（Attention）機構**を追加。                                                                                      | - デコーダが、入力シーケンスのどの部分に注目すべきかを学習する。<br>- コンテキストベクトルが固定長でなくなるため、**情報損失が大幅に改善**される。<br>- 長いシーケンスでも高い精度を達成できる。 |
| Transformer           | RNNの再帰的な構造を持たず、**アテンション機構**のみで構成。                                                                                              | - **並列計算**が可能で、学習が非常に高速。<br>- 大規模なデータセットでの高い性能が実証されている。<br>- BERTやGPTといった大規模言語モデルの基礎となるアーキテクチャ。           |
| U-Net                 | **エンコーダ**: 畳み込みとプーリングで特徴を抽出しダウンサンプリング。<br>**デコーダ**: アップサンプリングで元の解像度に戻す。エンコーダからの**スキップ接続**で詳細な情報を伝達。                            | - **画像セグメンテーション**に特化したモデル。<br>- 空間情報を保つことで、ピクセル単位の正確な分類を可能にする。                                            |

## Transformerを構成する3つの主要なアテンション

| アテンションの種類                                     | 役割と仕組み                                                                  | 使用される場所             |
| --------------------------------------------- | ----------------------------------------------------------------------- | ------------------- |
| 自己アテンション<br>(Self-Attention)                  | 文章中の各単語が、同じ文中の他の単語との関連性を計算し、重要度を判断<br>→離れた単語間の関係性を効率的に学習                | エンコーダー内、デコーダー内      |
| ソース・ターゲット・アテンション<br>(Source-Target Attention) | デコーダーが翻訳などのタスクを行う際、エンコーダーが読み込んだ元の文章のどの部分に注目すべきかを学習<br>→入力と出力の正確な対応関係を確立 | デコーダー内              |
| 位置エンコーディング<br>(Positional Encoding)           | Transformerは単語を同時に処理するため、単語の順番が失われる<br>この問題を解決するため、各単語のベクトルに位置情報を付加     | 入力埋め込み（Embedding）の後 |

# オートエンコーダ
教師なし学習の一種で、入力データを圧縮し、そこから元のデータを再構築することを目的としたニューラルネットワーク

| 種類          | 説明                                                                               | 主な特徴と用途                                                                 |
| ----------- | -------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| 単純なオートエンコーダ | 入力データをそのまま出力として再構築することを目指す、最も基本的なモデル。エンコーダとデコーダから構成される。                          | 次元削減や特徴抽出に用いられる。復元誤差（再構築誤差）を最小化するように学習する。                               |
| 積層オートエンコーダ  | 複数の単純なオートエンコーダを積み重ねて構築されるモデル。各オートエンコーダの出力が次の層の入力となる。                             | より複雑な特徴を階層的に学習できる。深層学習の初期の事前学習によく用いられた。                                 |
| 変分オートエンコーダ  | 潜在空間を確率分布としてモデル化し、データ生成能力を持たせたオートエンコーダ。エンコーダは潜在表現の平均と分散を学習する。                    | 新しいデータを**生成**することが主な目的。画像や音声、テキストなどのデータを生成できる。確率的なアプローチにより、潜在空間が滑らかになる。 |
| 敵対的生成ネットワーク | オートエンコーダとは異なり、生成器（Generator） と識別器（Discriminator） という2つのネットワークが互いに競い合いながら学習するモデル。 | VAEよりも高品質でリアルなデータを生成できる。学習が不安定になりやすいという課題がある。画像生成やスタイル変換に広く使われる。        |

## 変分エンコーダの種類

| モデル名                                 | 説明                                                                       | 主な特徴と貢献                                                                                   |
| ------------------------------------ | ------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------- |
| VAE (Variational Autoencoder)        | 潜在空間をガウス分布としてモデル化し、データ生成能力を持たせたオートエンコーダです。エンコーダは潜在表現の平均と分散を学習します。        | 新しいデータを**生成**することが主な目的。確率的なアプローチにより、潜在空間が滑らかになる。                                          |
| VQ-VAE (Vector Quantized VAE)        | 離散的な潜在表現を持つVAEです。潜在空間の連続値を**コードブック**と呼ばれるベクトル集合の中の最も近いベクトルにマッピングします。     | 潜在空間の離散化により、より鮮明な画像生成を可能にする。音声合成や画像生成に広く応用される。                                            |
| infoVAE (Information Maximizing VAE) | VAEに**情報ボトルネック原理**を組み込み、潜在表現と入力データ間の相互情報量を最大化するように学習するモデルです。             | 潜在空間に、より多くの入力データに関する情報を保持しようとする。これにより、潜在表現の**解釈可能性**が向上し、再構築性能が高まる。                       |
| β-VAE (Beta-VAE)                     | VAEの目的関数に**β**というハイパーパラメータを追加したモデルです。βを調整することで、潜在空間の制約（潜在表現の多様性）を制御できます。 | βを大きくすると、より** disentangled （分離された）**な潜在表現を学習できる。これにより、データ生成時の特定の属性（例：顔の角度や照明）を個別に操作可能になる。 |

## 敵対生成ネットワークの種類

| モデル名                                 | 概要                                                                                                                        | 主な特徴と用途                                                                   |
| ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- |
| GAN (Generative Adversarial Network) | **生成器（Generator）** と**識別器（Discriminator）** という2つのネットワークが互いに競い合いながら学習するモデル。生成器は偽のデータを生成し、識別器はそれが本物か偽物かを識別する。               | オートエンコーダよりも高品質でリアルな画像を生成できる。しかし、学習が不安定で、**モード崩壊（Mode Collapse）** が起こりやすい。 |
| DCGAN (Deep Convolutional GAN)       | GANに**深層畳み込みネットワーク（CNN）** を導入したモデル。プーリング層をストライド畳み込みに置き換えたり、バッチ正規化を導入したりして、学習を安定させた。                                       | 画像生成の質を大幅に向上させた。GANの実用化を大きく前進させたモデル。                                      |
| pix2pix                              | **条件付きGAN (Conditional GAN)** の一種で、入力画像から出力画像を生成するモデル。例えば、白黒画像からカラー画像を生成したり、航空写真から地図を生成したりする。                             | 特定の入力（条件）に基づいて画像を生成できる。ペアとなる訓練データセットが必要。                                  |
| CycleGAN                             | **pix2pix**の改良版。入力と出力のペアがない場合でも、ドメイン間（例：馬とシマウマ）の画像変換を可能にする。**サイクル一貫性損失（Cycle Consistency Loss）** を導入することで、ペアデータの必要性をなくした。 | スタイル変換やセグメンテーションなどに幅広く応用される。                                              |
| Diffusion Model                      | GANとは全く異なるアプローチで画像を生成するモデル。画像にノイズを段階的に加えていき、その逆プロセス（ノイズ除去）を学習することで画像を生成する。                                                | GANに比べて**学習が安定**しており、高品質で多様な画像を生成できる。画像生成の分野でGANに代わる主流な技術になりつつある。         |
# 音声処理

| 項目名                                         | 説明                                                                       | 役割/関連技術                                                     |
| ------------------------------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------------- |
| A-D変換                                       | アナログ音声信号をデジタルデータに説明変換するプロセス。                                             | **PCM（パルス符号変調）**は、この変換で最も一般的な手法。                            |
| 高速フーリエ変換 (FFT)                              | デジタル化された音声信号を、時間領域から**周波数領域**に変換するアルゴリズム。                                | 音声に含まれる各周波数の強さを分析する。周波数スペクトルの滑らかな輪郭が**スペクトル包絡**。            |
| フォルマント                                      | スペクトル包絡上に現れる、特にエネルギーが集中している周波数のピーク。その周波数を**フォルマント周波数**と呼ぶ                | 母音（あ、い、う、え、お）の識別に重要な特徴。                                     |
| メル尺度とMFCC                                   | 人間の聴覚が感じる音の高さをモデル化した非線形的な尺度（メル尺度）。**MFCC**は、このメル尺度に基づいて音声の特徴を抽出・圧縮した特徴量。 | 音声認識において、音声を特徴量としてモデルに入力するために広く使われる。                        |
| 音韻と音素                                       | **音韻**は、ある言語における音の体系や規則。**音素**は、意味を区別する最小単位（例：日本語の/a/）。                  | 音声認識モデルは、音素の並びを学習し、単語や文に変換する。                               |
| 隠れマルコフモデル (HMM)                             | 音声認識でかつて主流だった統計的モデル。観測された音響データ（音声）から、隠された状態（音素の並び）を推定する。                 | 現代の深層学習ベースのモデルに置き換わりつつある。                                   |
| 音声認識エンジン                                    | 音声をテキストに変換するシステム全体。                                                      | AlexaやSiriがこのシステムを利用。                                       |
| CTC (Connectionist Temporal Classification) | 時系列データ（音声）のラベル付けを学習するための**損失関数**。                                        | RNNやTransformerと組み合わせて使用され、入力と出力の長さが異なるタスクでアライメントを自動的に学習する。 |
| WaveNet                                     | Googleが開発した音声合成モデル。テキストから音声波形を直接生成する。                                    | 人間のように自然で高品質な音声を合成できる。                                      |
| 話者識別                                        | 音声信号そのものから、それが**誰の声であるかを特定**する技術。                                        | 生体認証やセキュリティ、コールセンターでの本人確認などに利用される。                          |
# 自然言語処理（Natural Language Processing: NLP）
人間が日常的に使っている言葉をコンピュータに理解・生成させるための技術

| 手法名                                    | 概要                                                                               | 主な特徴と用途                                                                          |
| -------------------------------------- | -------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| N-gram                                 | テキストを、連続するN個の単語の並びで区切る手法                                                         | 単語の**順序情報**をある程度保持できる。主に**言語モデル**や**スペルチェック**に用いられる。Nが大きくなると、組み合わせが爆発的に増える。      |
| **Bag-of-Words**                       | 文書内の単語の出現回数を数え、その頻度をベクトルとして表現する手法。単語の**順序は考慮しない**。                               | シンプルで実装が容易。大規模な語彙を持つと、疎な（ゼロが多い）巨大なベクトルになる。**文書分類**や**情報検索**の初期段階で使われる。           |
| TF-IDF                                 | 単語の重要度を計算する手法。文書内での出現頻度（**TF**）が高く、かつ、他の多くの文書ではあまり出現しない（**IDF**）単語ほど、その重要度が高くなる。 | Bag-of-Wordsの欠点（「the」や「is」のような頻出単語の重みが不当に高くなる）を改善する。**情報検索**や**キーワード抽出**に広く使われる。 |
| Word2Vec                               | 単語を、その周辺の単語から予測するように学習し、意味的な情報を持つ**ベクトル**として表現する手法。                              | 単語の**意味的な類似性**をベクトル空間で捉える。例：「王様 - 男性 + 女性 = 女王様」。                                |
| CBOW (Continuous Bag-of-Words)         | Word2Vecの一種。周囲の単語から**ターゲット単語**を予測するように学習する。                                      | Skip-gramより学習が速く、頻出単語によく機能する。                                                    |
| Skip-gram                              | Word2Vecの一種。**ターゲット単語**から周囲の単語を予測するように学習する。                                      | CBOWより学習は遅いが、**稀な単語**や**新しい単語**のベクトルをより正確に学習できる。                                 |
| fastText                               | Facebookが開発した単語埋め込み手法。単語を**N-gram**に分割し、それぞれのN-gramをベクトル化して学習する。                 | 誤字や未知語に対しても、部分的な情報から意味を推定できる。**形態論**が豊かな言語（日本語、ドイツ語など）に有効。                       |
| ELMo (Embeddings from Language Models) | **文脈**を考慮して、同じ単語でも異なるベクトルを生成する手法。双方向LSTMを使って単語の埋め込みを生成する。                        | **文脈に応じた**単語の意味を捉えることができる。例えば、「bank」が「銀行」か「川岸」かによって異なるベクトルを生成する。                 |
# 事前学習モデルの進化 

| モデル名                                                           | 開発者    | 主な特徴と貢献                                                                                                                       |
| -------------------------------------------------------------- | ------ | ----------------------------------------------------------------------------------------------------------------------------- |
| BERT (Bidirectional Encoder Representations from Transformers) | Google | 文脈を**双方向**から学習する**エンコーダ**ベースのモデル。**マスク言語モデル（MLM）** と**次文予測（NSP）** というタスクで事前学習されます。文の深い理解が必要なタスク（例：質問応答、感情分析）で高い性能を発揮します。      |
| GPT (Generative Pre-trained Transformer)                       | OpenAI | 文を左から右へ順に生成する**デコーダ**ベースのモデル。**自己回帰**（Autoregressive）方式で事前学習されます。主に文章生成、要約、翻訳など、新しいテキストを生成するタスクに優れています。                       |
| PaLM (Pathways Language Model)                                 | Google | 大規模なTransformerベースの言語モデル。**Pathways**という、複数のタスクを効率的に学習するための新しいAIシステム上で構築されました。非常に多くのパラメータとデータで学習されており、複雑な推論や多言語タスクに強みを持っています。 |
# マルチモーダルモデル
複数の異なるデータ形式（モダリティ）を組み合わせて処理・理解するAIモデル

| モデル名       | 開発者      | 主な特徴と能力                                                                                                                                 |
| ---------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| DALL-E     | OpenAI   | テキストの指示から画像を生成するモデル。非常に多様なスタイルや概念を組み合わせた画像を創造できます。テキストと画像の関係性を学習することで、創造的な画像生成を可能にしました。                                                 |
| Flamingo   | DeepMind | 画像や動画とテキストを組み合わせた入力を処理できるモデル。チャットボットのように、画像に関する質問にテキストで答えたり、画像にキャプションをつけたりできます。**大規模言語モデル（LLM）** と視覚エンコーダを組み合わせることで、画像とテキストの対話を可能にしました。 |
| Unified-IO | Google   | 画像、テキスト、音声など複数のモダリティを**単一のモデル**で統一的に処理するモデル。さまざまなタスク（画像生成、キャプション生成、物体検出など）を、入力と出力の形式を統一して学習します。これにより、タスク間の知識共有を可能にしました。                 |
| GPT-4o     | OpenAI   | テキスト、音声、画像の3つのモダリティをネイティブに処理するモデル。音声をリアルタイムで理解し、応答することができ、画像の内容についても対話できます。異なるモダリティを統合した推論能力が非常に高いです。                                   |
| CLIP       | OpenAI   | テキストと画像の関連性を学習するモデル。画像とテキストのペアを大量に学習することで、**画像の内容をテキストで説明**したり、**テキストから関連画像を検索**したりする能力を獲得しました。画像分類や検索タスクに広く応用されています。                   |
# 