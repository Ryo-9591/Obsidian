# AIの誕生

| 項目                                             | 説明                                                                       | 例                           |
| ---------------------------------------------- | ------------------------------------------------------------------------ | --------------------------- |
| レベル1<br><font color="#eeece1">ルールベースAI</font>  | すべての振る舞いがあらかじめ決められている<br>シンプルな制御システム                                     | エアコン、自動販売機                  |
| レベル2<br>古典的AI                                  | 探索・推論・知識データを用いて、<br>状況に応じた振る舞い                                           | 将棋やチェスのAI<br>エキスパートシステム     |
| レベル3<br><font color="#ffff00">機械学習AI</font>    | <font color="#eeece1">非常に多くのサンプルデータをもとに、<br>入力と出力の関係を学習するシステム</font><br> | レコメンデーションシステム<br>迷惑メールフィルター |
| レベル4<br><font color="#ffff00">ディープラーニング</font> | データから特徴量を自律的に学習する、<br>より高度なシステム                                          | 顔認証　音声認識　画像生成AI             |

| 年    | 内容                                                                                                                                                                                      |
| ---- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1946 | 世界初の汎用電子計算機<font color="#ffff00">エニアック(ENIAC) </font>誕生                                                                                                                                 |
| 1956 | ダートマス会議<br>①人工知能という言葉が<font color="#ffff00">ジョン・マッカーシー</font>によって初めて使用<br>②ハーバート・サイモン、アレン・ニューウェル、クリフ・ショーの数学の定理を自動的に証明する初期のAIプログラム<font color="#ffff00">ロジック・セオリスト</font> デモンストレーション<br> |

# 強いAIと弱いAIの提唱（ジョンサール）

| 項目   | 説明                    | 例        |
| ---- | --------------------- | -------- |
| 強いAI | 汎用的人工知能               | 自我を持ったAI |
| 弱いAI | 特化型人工知能<br>→中国語の部屋などで | AlphaGo  |

# 第一次AIブーム（1950年代後半〜1960年代）
<font color="#ffff00">トイ・プロブレム</font>（パズルや迷路探索といった限られた問題）で成果を上げる
現実世界の複雑な情報や未知の状況を扱うことができない<font color="#ffff00">フレーム問題</font>（1969）に直面

| 項目                                | 内容                                                                                                           |
| --------------------------------- | :----------------------------------------------------------------------------------------------------------- |
| ハノイの塔                             | ルール<br>1.  一度に一つの円盤しか動かせない。<br>2. 小さい円盤の上に大きい円盤を乗せてはいけない。<br>3. 3本ある杭のどれを使ってもよい。<br>最小手数で解くには、2n−1回の手数が必要となる |
| STRIPS                            | 前提・行動・結果を記述するプランニング                                                                                          |
| 1968～1970年<br>SHRDLU（テリー・ウィノグラード） | プランニングを「積み木の世界」で完全再現<br>自然言語だけで積み木を作れた                                                                       |
| 1984年<br>Cycプロジェクト（ダグ・レナート）       | 常識を記号の形で表現し、それを基にコンピュータが論理的な推論を行えるようにすることを目指す                                                                |


# ボードゲームパターン数

| ゲーム | パターン数  |
| --- | ------ |
| オセロ | 10^60  |
| チェス | 10^120 |
| 将棋  | 10^220 |
| 囲碁  | 10^360 |

# 探索アルゴリズム

|             |                                                                      | 図                                    |
| ----------- | -------------------------------------------------------------------- | ------------------------------------ |
| 幅優先探索       | 根から近いノードから順に探索<br>すべてのノードをキューに保持する必要がある<br>→メモリ不足になる                 | ![[Pasted image 20250901174929.png]] |
| 深さ優先探索      | 根からできるだけ深く探索<br>一度に一つの経路上のノードだけを記憶すればよい<br>→メモリ不足になりにくい<br>          | ![[Pasted image 20250901174941.png]] |
| Mini-Max法   | 相手も自分も最善を尽くすという前提で、ゲームのすべての可能な局面をツリー状に展開し、最終的な評価値が最大（ミニ）になるような手を選ぶ方法 | ![[Pasted image 20250901105639.png]] |
| Alpha-Beta法 | ミニマックス法を改良したもので、明らかに不利な局面の探索を途中で打ち切る                                 | ![[Pasted image 20250901105752.png]] |
| ヒューリスティック探索 | ヒューリスティクスを使って、より効率的に探索を進める方法                                         |                                      |
| モンテカルロ木探索   | ランダムな試行（シミュレーション）を繰り返して、最も有望な手を見つけ出します。                              |                                      |

## AI効果
かつて人工知能（AI）だと考えられていた技術が、普及して当たり前になると、もはやAIとはみなされなくなる現象
## 1950年：チューリングテスト（アラン・チューリング）
機械が人間と同等の知的振る舞いを示せるかを判定するためのテスト
## 1991年：ローブナーコンテスト（Loebner Prize） 
 チューリングテストの実践版として始まった年次コンテスト
## 1966年：イライザ（ELIZA） （ジョセフ・ワイゼンバウム） 人工無能
初期の対話プログラムです。キーワードに反応して質問を返すという単純な仕組み
イライザ効果：イライザを人間と勘違いする現象
## 身体性
知能獲得するには<font color="#ffff00">身体が不可欠</font>とする考え方
## 2045年：シンギュラリティ（レイ・カーツワイル）
AIが自らより優れたAIを開発し、そのAIがさらに優れたAIを開発するという「知能の爆発的連鎖」が起こることで、人類の知性を超える超知能（Superintelligence）が誕生するという考え

# 第二次AIブーム（1980年代〜）
知識獲得のボトルネック・シンボルグラウンディング問題
## エキスパートシステム
知識ベースと推論エンジンにより構成

| 名前      | 年    | 開発者            | 説明                                                                               |
| ------- | ---- | -------------- | -------------------------------------------------------------------------------- |
| DENDRAL | 1960 | エドワード・ファイゲンバウム | 質量スペクトルデータから、未知の有機化合物の分子構造を推定するシステム。専門家が利用する「ヒューリスティック（発見的手法）」をルール化して組み込んだ。      |
| MYCIN   | 1970 | スタンフォード大学      | 血液感染症の原因菌を特定し、治療薬を推奨するシステム。不確実な情報（発熱、症状など）から確率的に推論を行う「**確信度係数**」の概念を導入したことで知られる。 |
# オントロジー
意味ネットワークなどで用いられる知識の結び付け方の規則

| オントロジー種類       | 内容                                            |
| -------------- | --------------------------------------------- |
| ライトウェイト・オントロジー | 概念の分類（タクソノミー）や単純な語彙関係に焦点を当てた、比較的単純な構造のオントロジー。 |
| ヘビーウェイト・オントロジー | 複雑な概念間の関係性や制約、論理的な推論ルールを詳細に記述する、複雑な構造のオントロジー。 |

| 関連出来事      | 年    | 内容                                                                     |
| ---------- | ---- | ---------------------------------------------------------------------- |
| Cycプロジェクト  | 1984 | 人間のすべての一般常識をデータベース化し、知識ベースを構築することで人間と同等の推論システムを目指しました 。                |
| セマンティックウェブ |      | ウェブ上のデータをコンピュータが理解・解釈できるようにするための技術や概念の総称。オントロジーは、このセマンティックウェブの中核技術の一つ。 |
| ワトソン（IBM）  | 2011 | アメリカのクイズ番組ジョパディーで勝利(ライトオントロジーを使用)<br>質問に対して高速で検索しているだけ                 |
| 東ロボ君       | 2011 | 東京大学の合格を目指すも意味理解に苦しみ凍結                                                 |
# 第三次AIブーム（2000年代後半〜現在） 機械学習・特徴表現学習の時代

| 種類     | 内容            |
| ------ | ------------- |
| 教師あり学習 | ラベルを学習しラベルを予想 |
| 教師なし学習 | 構造やパターンを抽出    |
| 強化学習   | 将来の報酬を最大化     |
# 教師あり学習

## 回帰

| 項目          | 特徴                                                                             |
| ----------- | ------------------------------------------------------------------------------ |
| 線形回帰        | 入力特徴量と出力の間を、線形な関係でモデル化する最も基本的な手法。重み（係数）とバイアス（切片）を学習                            |
| 多項式回帰       | 線形回帰を拡張したもので、入力特徴量の多項式（2次、3次など）を使って非線形な関係をモデル化                                 |
| リッジ回帰       | 線形回帰に**L2正則化**（L2 regularization）を導入した手法。係数を小さくすることで、過学習を防ぎ、モデルの汎化能力を高める       |
| ラッソ回帰       | 線形回帰に**L1正則化**（L1 regularization）を導入した手法。重要でない特徴量の係数を強制的にゼロにすることで、特徴量選択を行う     |
| 決定木回帰       | データを階層的に分割し、各領域の平均値を出力として予測する手法。シンプルなルールベースのモデルを構築                             |
| ランダムフォレスト回帰 | 複数の決定木を組み合わせて予測を行う**アンサンブル学習**の一種。各決定木の予測の平均値を出力とする。                           |
| サポートベクター回帰  | サポートベクターマシン（SVM）を回帰に応用したもの。マージン内にできるだけ多くのデータポイントを収め、マージン外の誤差を最小化するようにモデルを学習する。 |


| 項目  | 特徴                                     |
| --- | -------------------------------------- |
| 単回帰 | 1つの独立変数（説明変数） を用いて、1つの従属変数（目的変数） を予測   |
| 重回帰 | 2つ以上の独立変数（説明変数） を用いて、1つの従属変数（目的変数） を予測 |

## 分類

| 項目          | 特徴                                                                                                                     |
| ----------- | ---------------------------------------------------------------------------------------------------------------------- |
| ロジスティック回帰   | 出力が0から1の間の確率であるため、分類問題によく用いられる。この確率をもとにクラスを判別する。                                                                       |
| サポートベクターマシン | マージン内にできるだけ多くのデータポイントを収め、マージン外の誤差を最小化するようにモデルを学習する。<br>**カーネルトリック**とそれを実現する**カーネル関数**によって、線形分離できない複雑なデータに対しても高い分類性能を発揮 |
| ランダムフォレスト   | 複数の決定木をランダムに作成し、それぞれの予測結果を多数決で最終的なクラスを決定する**アンサンブル学習**                                                                 |
| K-近傍法       | 予測したいデータに最も近い「K個」の訓練データを探し、それらのデータの多数決でクラスを決定する。                                                                       |
| ナイーブベイズ     | ベイズの定理に基づいて、各特徴量が独立であると仮定して確率を計算し、最も確率の高いクラスを予測する。                                                                     |
| ニューラルネットワーク | 脳の神経回路を模倣したモデル。入力層、中間層（隠れ層）、出力層から構成され、複雑な非線形関係を学習できる。                                                                  |

# アンサンブル学習

| 項目      | 特徴                                                                                           | 例                                     |
| ------- | -------------------------------------------------------------------------------------------- | ------------------------------------- |
| バギング    | 訓練データから**ブートストラップ法**（復元抽出）で複数のサブセットを作成し、それぞれでモデルを学習させる。最終的な予測は、**多数決（分類）** や**平均（回帰）** で決める。 | ランダムフォレスト                             |
| ブースティング | 性能の低いモデルを**逐次的**に学習させる。前のモデルが誤って予測したデータに重みを付けて、次のモデルがそのデータをより重視して学習する。                       | **AdaBoost**、**XGBoost**、**LightGBM** |
| スタッキング  | 複数の異なる種類のモデル（例：決定木、SVM）を学習させ、それらの予測結果を新たな特徴量として、**メタモデル**（最終的なモデル）を学習させる。                    |                                       |

| 項目       | 内容                                                                    | 特徴                                               |
| -------- | --------------------------------------------------------------------- | ------------------------------------------------ |
| AdaBoost | 誤分類されたデータに重みを加算し、次の学習器がそのデータをより重視して学習する。                              | 比較的速いが、XGBoostやLightGBMには劣る。                     |
| XGBoost  | **勾配ブースティング**の一種。損失関数の勾配を使ってモデルを逐次的に最適化する。過学習を防ぐための正則化機能を持つ。          | 並列処理に対応しているため、AdaBoostより速い。                      |
| LightGBM | **勾配ブースティング**の一種。XGBoostよりも高速で、大規模データに強い。葉っぱ単位で成長する**リーフワイズ**な決定木を使う。 | 非常に高速。**ヒストグラムベース**のアプローチにより、XGBoostより数倍速いことがある。 |

# 教師なし学習

## クラスタリング　データをグループ分けすること

| 項目          | 内容                                                                    |
| ----------- | --------------------------------------------------------------------- |
| 階層なしクラスタリング | クラスター数が固定されており、階層的な関係は作られません。アルゴリズムが高速で、大規模データに適しています。                |
| 階層ありクラスタリング | データポイント間の距離に基づいて、ツリー構造（デンドログラム）を構築します。クラスター数は後から決定できますが、計算コストが高くなります。 |

| 項目       | 内容                                                                                                   | 特徴                                                                  |
| -------- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| k-menas法 | **階層なしクラスタリング**の代表的な手法。事前に指定したクラスター数 (k) に基づき、データポイントを最も近いクラスター中心（重心）に割り当てていく。                       | **高速**でシンプル。大規模データに有効。クラスターの形状が**球状**であると仮定する。クラスター数を事前に決める必要がある。   |
| ウォード法    | **階層ありクラスタリング（凝集型）** の手法の一つ。クラスター間の距離を、**合併による情報損失**の増加量として定義する。クラスターの重心からの平方和を最小化するようにクラスターを併合していく。 | クラスターのサイズがほぼ均等になる傾向がある。**凝集型**の中で最も人気があり、バランスの取れたクラスターを形成する。        |
| 最短距離法    | **階層ありクラスタリング（凝集型）** の手法の一つ。2つのクラスター間の距離を、**それぞれのクラスターに属する最も近いデータポイント間の距離**と定義する。                    | **チェーン効果 (chaining effect)** が起こりやすく、細長いクラスターを形成しやすい。ノイズや外れ値に非常に敏感。 |
## 主成分分析（PCA）　データの次元を減らすこと
### 特異値分解（SVD）
任意の行列を3つの行列に分解する手法
次元削減やデータ圧縮、ノイズ除去などに広く使われる強力なツール

特異値の大きい上位数個の成分だけを使って行列を再構築することで、元のデータの重要な特徴を保ちながら次元削減を行うことができる
### 多次元尺度構成法（MDS）
データ間の類似度や非類似度の情報だけを使って、データを低次元の空間に配置し直す可視化手法
![[Pasted image 20250901142102.png]]
### t-SNE（t-distributed Stochastic Neighbor Embedding）
高次元データを**2次元または3次元に圧縮**して、可視化するための手法
データ間の絶対的な距離は保持しないため、クラスタ間の距離は元のデータの距離を正確に表すものではない点に注意
![[Pasted image 20250901142317.png]]

---
# 協調フィルタリング
レコメンデーションシステムで最も広く使われる手法の一つです。ユーザーの行動データ（購入履歴や評価など）を利用して、**類似した嗜好を持つユーザー**や**類似したアイテム**を見つけ出し、レコメンドを行い
## コールドスタート問題
- **新規ユーザー**: 新規ユーザーは行動履歴がないため、システムがそのユーザーの嗜好を判断できず、適切なレコメンドができません。
- **新規アイテム**: 新規に追加された商品や映画は、まだ誰も評価や購入をしていないため、システムがそのアイテムを誰にレコメンドすべきか判断できません。
## コンテンツベースフィルタリング
**アイテム自身の属性**（例：映画のジャンル、俳優、監督など）に基づいてレコメンドを行う手法です。

たとえば、あなたがSF映画が好きなら、SFジャンルの別の映画をおすすめします。

---
# トピックモデル
文書の集合（コーパス）を分析し、その中に隠れている**トピック**を発見するための機械学習手法です。文書は、複数のトピックから構成されており、それぞれのトピックが特定の単語の確率分布を持っていると仮定
## 潜在的ディリクレ配分法
以下の仮定に基づいて、文書、トピック、単語の関係をモデル化
1. **各文書は、複数のトピックの組み合わせでできている。** たとえば、ある文書は「AI」トピックが70%、「プログラミング」トピックが30%で構成されている、といった具合です。
2. **各トピックは、複数の単語の組み合わせでできている。** たとえば、「AI」トピックは、「機械学習」「ディープラーニング」「アルゴリズム」といった単語がよく含まれる、といった具合です。

