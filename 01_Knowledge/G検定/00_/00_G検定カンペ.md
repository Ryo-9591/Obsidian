# AIの誕生

| 項目                                             | 説明                                                                       | 例                           |
| ---------------------------------------------- | ------------------------------------------------------------------------ | --------------------------- |
| レベル1<br><font color="#eeece1">ルールベースAI</font>  | すべての振る舞いがあらかじめ決められている<br>シンプルな制御システム                                     | エアコン、自動販売機                  |
| レベル2<br>古典的AI                                  | 探索・推論・知識データを用いて、<br>状況に応じた振る舞い                                           | 将棋やチェスのAI<br>エキスパートシステム     |
| レベル3<br><font color="#ffff00">機械学習AI</font>    | <font color="#eeece1">非常に多くのサンプルデータをもとに、<br>入力と出力の関係を学習するシステム</font><br> | レコメンデーションシステム<br>迷惑メールフィルター |
| レベル4<br><font color="#ffff00">ディープラーニング</font> | データから特徴量を自律的に学習する、<br>より高度なシステム                                          | 顔認証　音声認識　画像生成AI             |

| 年    | 内容                                                                                                                                                                                      |
| ---- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1946 | 世界初の汎用電子計算機<font color="#ffff00">エニアック(ENIAC) </font>誕生                                                                                                                                 |
| 1956 | ダートマス会議<br>①人工知能という言葉が<font color="#ffff00">ジョン・マッカーシー</font>によって初めて使用<br>②ハーバート・サイモン、アレン・ニューウェル、クリフ・ショーの数学の定理を自動的に証明する初期のAIプログラム<font color="#ffff00">ロジック・セオリスト</font> デモンストレーション<br> |

# 強いAIと弱いAIの提唱（ジョンサール）

| 項目   | 説明                    | 例        |
| ---- | --------------------- | -------- |
| 強いAI | 汎用的人工知能               | 自我を持ったAI |
| 弱いAI | 特化型人工知能<br>→中国語の部屋などで | AlphaGo  |

# 第一次AIブーム（1950年代後半〜1960年代）
<font color="#ffff00">トイ・プロブレム</font>（パズルや迷路探索といった限られた問題）で成果を上げる
現実世界の複雑な情報や未知の状況を扱うことができない<font color="#ffff00">フレーム問題</font>（1969）に直面

| 項目                                | 内容                                                                                                           |
| --------------------------------- | :----------------------------------------------------------------------------------------------------------- |
| ハノイの塔                             | ルール<br>1.  一度に一つの円盤しか動かせない。<br>2. 小さい円盤の上に大きい円盤を乗せてはいけない。<br>3. 3本ある杭のどれを使ってもよい。<br>最小手数で解くには、2n−1回の手数が必要となる |
| STRIPS                            | 前提・行動・結果を記述するプランニング                                                                                          |
| 1968～1970年<br>SHRDLU（テリー・ウィノグラード） | プランニングを「積み木の世界」で完全再現<br>自然言語だけで積み木を作れた                                                                       |
| 1984年<br>Cycプロジェクト（ダグ・レナート）       | 常識を記号の形で表現し、それを基にコンピュータが論理的な推論を行えるようにすることを目指す                                                                |


# ボードゲームパターン数

| ゲーム | パターン数  |
| --- | ------ |
| オセロ | 10^60  |
| チェス | 10^120 |
| 将棋  | 10^220 |
| 囲碁  | 10^360 |

# 探索アルゴリズム

|             |                                                                      | 図                                    |
| ----------- | -------------------------------------------------------------------- | ------------------------------------ |
| 幅優先探索       | 根から近いノードから順に探索<br>すべてのノードをキューに保持する必要がある<br>→メモリ不足になる                 | ![[Pasted image 20250901174929.png]] |
| 深さ優先探索      | 根からできるだけ深く探索<br>一度に一つの経路上のノードだけを記憶すればよい<br>→メモリ不足になりにくい<br>          | ![[Pasted image 20250901174941.png]] |
| Mini-Max法   | 相手も自分も最善を尽くすという前提で、ゲームのすべての可能な局面をツリー状に展開し、最終的な評価値が最大（ミニ）になるような手を選ぶ方法 | ![[Pasted image 20250901105639.png]] |
| Alpha-Beta法 | ミニマックス法を改良したもので、明らかに不利な局面の探索を途中で打ち切る                                 | ![[Pasted image 20250901105752.png]] |
| ヒューリスティック探索 | ヒューリスティクスを使って、より効率的に探索を進める方法                                         |                                      |
| モンテカルロ木探索   | ランダムな試行（シミュレーション）を繰り返して、最も有望な手を見つけ出します。                              |                                      |

## AI効果
かつて人工知能（AI）だと考えられていた技術が、普及して当たり前になると、もはやAIとはみなされなくなる現象
## 1950年：チューリングテスト（アラン・チューリング）
機械が人間と同等の知的振る舞いを示せるかを判定するためのテスト
## 1991年：ローブナーコンテスト（Loebner Prize） 
 チューリングテストの実践版として始まった年次コンテスト
## 1966年：イライザ（ELIZA） （ジョセフ・ワイゼンバウム） 人工無能
初期の対話プログラムです。キーワードに反応して質問を返すという単純な仕組み
イライザ効果：イライザを人間と勘違いする現象
## 身体性
知能獲得するには<font color="#ffff00">身体が不可欠</font>とする考え方
## 2045年：シンギュラリティ（レイ・カーツワイル）
AIが自らより優れたAIを開発し、そのAIがさらに優れたAIを開発するという「知能の爆発的連鎖」が起こることで、人類の知性を超える超知能（Superintelligence）が誕生するという考え

# 第二次AIブーム（1980年代〜）
知識獲得のボトルネック・シンボルグラウンディング問題
## エキスパートシステム
知識ベースと推論エンジンにより構成

| 名前      | 年    | 開発者            | 説明                                                                               |
| ------- | ---- | -------------- | -------------------------------------------------------------------------------- |
| DENDRAL | 1960 | エドワード・ファイゲンバウム | 質量スペクトルデータから、未知の有機化合物の分子構造を推定するシステム。専門家が利用する「ヒューリスティック（発見的手法）」をルール化して組み込んだ。      |
| MYCIN   | 1970 | スタンフォード大学      | 血液感染症の原因菌を特定し、治療薬を推奨するシステム。不確実な情報（発熱、症状など）から確率的に推論を行う「**確信度係数**」の概念を導入したことで知られる。 |
# オントロジー
意味ネットワークなどで用いられる知識の結び付け方の規則

| オントロジー種類       | 内容                                            |
| -------------- | --------------------------------------------- |
| ライトウェイト・オントロジー | 概念の分類（タクソノミー）や単純な語彙関係に焦点を当てた、比較的単純な構造のオントロジー。 |
| ヘビーウェイト・オントロジー | 複雑な概念間の関係性や制約、論理的な推論ルールを詳細に記述する、複雑な構造のオントロジー。 |

| 関連出来事      | 年    | 内容                                                                     |
| ---------- | ---- | ---------------------------------------------------------------------- |
| Cycプロジェクト  | 1984 | 人間のすべての一般常識をデータベース化し、知識ベースを構築することで人間と同等の推論システムを目指しました 。                |
| セマンティックウェブ |      | ウェブ上のデータをコンピュータが理解・解釈できるようにするための技術や概念の総称。オントロジーは、このセマンティックウェブの中核技術の一つ。 |
| ワトソン（IBM）  | 2011 | アメリカのクイズ番組ジョパディーで勝利(ライトオントロジーを使用)<br>質問に対して高速で検索しているだけ                 |
| 東ロボ君       | 2011 | 東京大学の合格を目指すも意味理解に苦しみ凍結                                                 |
# 第三次AIブーム（2000年代後半〜現在） 機械学習・特徴表現学習の時代

| 種類     | 内容            |
| ------ | ------------- |
| 教師あり学習 | ラベルを学習しラベルを予想 |
| 教師なし学習 | 構造やパターンを抽出    |
| 強化学習   | 将来の報酬を最大化     |
# 教師あり学習

## 回帰

| 項目          | 特徴                                                                             |
| ----------- | ------------------------------------------------------------------------------ |
| 線形回帰        | 入力特徴量と出力の間を、線形な関係でモデル化する最も基本的な手法。重み（係数）とバイアス（切片）を学習                            |
| 多項式回帰       | 線形回帰を拡張したもので、入力特徴量の多項式（2次、3次など）を使って非線形な関係をモデル化                                 |
| リッジ回帰       | 線形回帰に**L2正則化**（L2 regularization）を導入した手法。係数を小さくすることで、過学習を防ぎ、モデルの汎化能力を高める       |
| ラッソ回帰       | 線形回帰に**L1正則化**（L1 regularization）を導入した手法。重要でない特徴量の係数を強制的にゼロにすることで、特徴量選択を行う     |
| 決定木回帰       | データを階層的に分割し、各領域の平均値を出力として予測する手法。シンプルなルールベースのモデルを構築                             |
| ランダムフォレスト回帰 | 複数の決定木を組み合わせて予測を行う**アンサンブル学習**の一種。各決定木の予測の平均値を出力とする。                           |
| サポートベクター回帰  | サポートベクターマシン（SVM）を回帰に応用したもの。マージン内にできるだけ多くのデータポイントを収め、マージン外の誤差を最小化するようにモデルを学習する。 |


| 項目  | 特徴                                     |
| --- | -------------------------------------- |
| 単回帰 | 1つの独立変数（説明変数） を用いて、1つの従属変数（目的変数） を予測   |
| 重回帰 | 2つ以上の独立変数（説明変数） を用いて、1つの従属変数（目的変数） を予測 |

## 分類

| 項目          | 特徴                                                                                                                     |
| ----------- | ---------------------------------------------------------------------------------------------------------------------- |
| ロジスティック回帰   | 出力が0から1の間の確率であるため、分類問題によく用いられる。この確率をもとにクラスを判別する。                                                                       |
| サポートベクターマシン | マージン内にできるだけ多くのデータポイントを収め、マージン外の誤差を最小化するようにモデルを学習する。<br>**カーネルトリック**とそれを実現する**カーネル関数**によって、線形分離できない複雑なデータに対しても高い分類性能を発揮 |
| ランダムフォレスト   | 複数の決定木をランダムに作成し、それぞれの予測結果を多数決で最終的なクラスを決定する**アンサンブル学習**                                                                 |
| K-近傍法       | 予測したいデータに最も近い「K個」の訓練データを探し、それらのデータの多数決でクラスを決定する。                                                                       |
| ナイーブベイズ     | ベイズの定理に基づいて、各特徴量が独立であると仮定して確率を計算し、最も確率の高いクラスを予測する。                                                                     |
| ニューラルネットワーク | 脳の神経回路を模倣したモデル。入力層、中間層（隠れ層）、出力層から構成され、複雑な非線形関係を学習できる。                                                                  |

# アンサンブル学習

| 項目      | 特徴                                                                                           | 例                                     |
| ------- | -------------------------------------------------------------------------------------------- | ------------------------------------- |
| バギング    | 訓練データから**ブートストラップ法**（復元抽出）で複数のサブセットを作成し、それぞれでモデルを学習させる。最終的な予測は、**多数決（分類）** や**平均（回帰）** で決める。 | ランダムフォレスト                             |
| ブースティング | 性能の低いモデルを**逐次的**に学習させる。前のモデルが誤って予測したデータに重みを付けて、次のモデルがそのデータをより重視して学習する。                       | **AdaBoost**、**XGBoost**、**LightGBM** |
| スタッキング  | 複数の異なる種類のモデル（例：決定木、SVM）を学習させ、それらの予測結果を新たな特徴量として、**メタモデル**（最終的なモデル）を学習させる。                    |                                       |

| 項目       | 内容                                                                    | 特徴                                               |
| -------- | --------------------------------------------------------------------- | ------------------------------------------------ |
| AdaBoost | 誤分類されたデータに重みを加算し、次の学習器がそのデータをより重視して学習する。                              | 比較的速いが、XGBoostやLightGBMには劣る。                     |
| XGBoost  | **勾配ブースティング**の一種。損失関数の勾配を使ってモデルを逐次的に最適化する。過学習を防ぐための正則化機能を持つ。          | 並列処理に対応しているため、AdaBoostより速い。                      |
| LightGBM | **勾配ブースティング**の一種。XGBoostよりも高速で、大規模データに強い。葉っぱ単位で成長する**リーフワイズ**な決定木を使う。 | 非常に高速。**ヒストグラムベース**のアプローチにより、XGBoostより数倍速いことがある。 |

# 教師なし学習

## クラスタリング　データをグループ分けすること

| 項目          | 内容                                                                    |
| ----------- | --------------------------------------------------------------------- |
| 階層なしクラスタリング | クラスター数が固定されており、階層的な関係は作られません。アルゴリズムが高速で、大規模データに適しています。                |
| 階層ありクラスタリング | データポイント間の距離に基づいて、ツリー構造（デンドログラム）を構築します。クラスター数は後から決定できますが、計算コストが高くなります。 |

| 項目       | 内容                                                                                                   | 特徴                                                                  |
| -------- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| k-menas法 | **階層なしクラスタリング**の代表的な手法。事前に指定したクラスター数 (k) に基づき、データポイントを最も近いクラスター中心（重心）に割り当てていく。                       | **高速**でシンプル。大規模データに有効。クラスターの形状が**球状**であると仮定する。クラスター数を事前に決める必要がある。   |
| ウォード法    | **階層ありクラスタリング（凝集型）** の手法の一つ。クラスター間の距離を、**合併による情報損失**の増加量として定義する。クラスターの重心からの平方和を最小化するようにクラスターを併合していく。 | クラスターのサイズがほぼ均等になる傾向がある。**凝集型**の中で最も人気があり、バランスの取れたクラスターを形成する。        |
| 最短距離法    | **階層ありクラスタリング（凝集型）** の手法の一つ。2つのクラスター間の距離を、**それぞれのクラスターに属する最も近いデータポイント間の距離**と定義する。                    | **チェーン効果 (chaining effect)** が起こりやすく、細長いクラスターを形成しやすい。ノイズや外れ値に非常に敏感。 |
## 主成分分析（PCA）　データの次元を減らすこと

| 項目       | 内容                                                                        | 特徴                                                                                                                  |                                      |
| -------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------ |
| 特異値分解    | 行列を3つの行列（特異ベクトル、特異値）に分解する線形代数の手法。**主成分分析（PCA）** の実用的な計算方法として広く用いられる。      | - **線形**な手法。<br>- 元のデータの分散（バラつき）を最大化するように次元削減を行う。<br>- **情報圧縮**や**ノイズ除去**、**レコメンデーションシステム**に用いられる。                  |                                      |
| 多次元尺度構成法 | データ間の**距離や類似度**を保ちながら、高次元データを低次元空間にマッピングする手法。                             | データの**相対的な位置関係**を重視する。<br>- データそのものではなく、**データ間の距離行列**を入力として使用する。<br>- **心理学**や**マーケティング**分野で、消費者の好みなどを可視化するのに用いられる。 | ![[Pasted image 20250901142102.png]] |
| t-SNE    | 高次元空間でのデータ間の類似性を、低次元空間での確率分布として表現する**非線形**な次元削減手法。特に、**局所的な構造**を保つことに優れる。 | **非線形**な手法。<br>- クラスターやグループなどの**局所的な構造を非常に明確に可視化**できる。<br>- **データ可視化**に特化しており、機械学習の前処理としてはあまり使われない。                 | ![[Pasted image 20250901142317.png]] |

# 協調フィルタリング
多くのユーザーの行動や好みを利用して、新しいアイテムを推薦するレコメンデーション手法

| 項目               | 内容                                                         | 特徴                                                                           |
| ---------------- | ---------------------------------------------------------- | ---------------------------------------------------------------------------- |
| ユーザーベース協調フィルタリング | ターゲットユーザーと似た好みを持つ他のユーザーを見つけ出し、そのユーザーが良い評価をしたアイテムを推薦する手法です。 | 直感的で分かりやすい。ユーザー数が増加すると計算コストが急増する。新しいユーザーへの推薦が難しい「コールドスタート問題」に弱い。             |
| アイテムベース協調フィルタリング | ユーザーが過去に良い評価をしたアイテムと似たアイテムを推薦する手法です。アイテム間の類似度を計算する。        | ユーザーベースに比べて安定した推薦が可能。新しいアイテムが参入した際のコールドスタート問題に弱い。Netflixの推薦システムで使われたことで知られる。 |
| モデルベース協調フィルタリング  | データから潜在的なパターンや特徴を学習する機械学習モデルを構築し、それに基づいて推薦を行う手法です。         | スケーラビリティが高く、大規模データに対応できる。予測精度が高い。代表的な手法に行列分解がある。                             |
# トピックモデル
文書の集合（コーパス）の中から、潜在的な「トピック」を発見する教師なし学習の手法

| 項目          | 内容                                                                               |
| ----------- | -------------------------------------------------------------------------------- |
| 潜在的意味解析     | 文書と単語の共起行列に**特異値分解（SVD）** を適用することで、文書と単語を共通の潜在空間（トピック空間）に写像する手法です。               |
| 確率的潜在意味解析   | 文書がトピックの混合であり、各トピックが単語の確率分布であると仮定する**確率的モデル**です。文書と単語の共起を、トピックを介した確率としてモデル化します。  |
| 潜在的ディリクレ配分法 | **PLSA**を拡張した**ベイジアンモデル**です。各文書がトピックの確率分布の混合として生成され、各トピックが単語の確率分布として生成されると仮定します。 |
# 強化学習

| 項目           | 内容                                                                                                               |
| ------------ | ---------------------------------------------------------------------------------------------------------------- |
| バンディットアルゴリズム | 限られたリソースの中で、**探索（未知の選択肢を試す）** と**活用（既知の最良の選択肢を選ぶ）** のバランスを効率的にとるためのアルゴリズム                                        |
| ϵ-Greedy     | 確率εで**探索**を行い、確率1-εで**活用**を行うという、バンディット問題の解決策の一つ                                                                 |
| マルコフ決定過程     | 強化学習の多くの問題を数学的に定式化するためのフレームワーク<br><br>マルコフ性（Markov Property）<br>「ある時点の未来の状態は、現在の状態にのみ依存し、それまでの過去の履歴には依存しない」という性質 |

| 項目          | 内容                                                                                | 特徴と用途                                                                                                                                                                            |
| ----------- | --------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Q学習         | 価値ベース学習の代表。状態-行動ペアの**価値（Q値）** を学習し、そのQ値の最大化を目指す。行動選択には$\epsilon$-greedy法がよく用いられる。 | - オフポリシー（方策とは異なる行動から学習可能）。<br>- シンプルで実装が容易。<br>- 離散的な状態と行動空間を持つ問題に有効                                                                                                             |
| SARSA       | Q学習と同様の価値ベース学習。行動選択に用いた方策に基づいてQ値を更新する。                                            | - オンポリシー（行動に用いた方策から学習）。<br>- Q学習より安全な学習が期待される。                                                                                                                                   |
| DQN         | Q学習と**ディープラーニング**を組み合わせたもの。Q値をニューラルネットワークで近似することで、状態空間が膨大な問題にも対応可能。               | - **経験再生 (Experience Replay)**: 過去の経験をメモリに蓄え、そこからランダムにサンプリングして学習に用いる。これにより、データの相関が低減し、学習が安定する。 <br> - **ターゲットネットワーク**: 学習に用いるQ値の更新目標となるネットワークを、一定期間ごとに固定する。これにより、学習目標が安定し、発散を防ぐ。 |
| Double DQN  | **ターゲットQ値**の過大評価（overestimation）を抑制するために、Q値の計算方法を改良したDQN。                         | **Q値の過大評価を抑制**: 次の行動を評価する際に、オンラインネットワーク（最新のネットワーク）で行動を選び、ターゲットネットワークでその行動のQ値を評価する。これにより、過大評価が減り、安定した学習が可能になる。                                                                    |
| Dueling DQN | ネットワークのアーキテクチャを改良したDQN。                                                           | - **価値と優位性の分離**: ネットワークを「状態の価値 (V(s))」と「各行動の優位性 (Advantage, A(s,a))」に分離して学習する。これにより、各行動の価値をより正確に評価できる。                                                                           |
| Rainbow     | 上記の複数の改良点（DDQN, PER, Dueling DQN, C51など）を組み合わせたDQN。                               | - **高い性能**: それぞれの改良点の相乗効果により、単一のDQNよりも高い性能を発揮する。                                                                                                                                 |
| REINFORCE   | 方策勾配法（Policy Gradient Method）の代表。行動の価値ではなく、**行動方策（ポリシー）** を直接学習する。                | - 連続的な行動空間を持つ問題に適用可能。<br>- 方策ベース学習の最も基本的なアルゴリズム。                                                                                                                                 |
# モデルの評価

| 項目       | 内容                                                                                           | 特徴と用途                                                     |
| -------- | -------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| ホールドアウト法 | データを一度だけ訓練用とテスト用に分割し、テストデータでモデルを評価します。                                                       | シンプルで実装が容易。計算コストが低い。                                      |
| K-分割交差検証 | データをK個の等しいサイズのサブセットに分割します。そのうちの1つをテストデータ、残りのK−1個を訓練データとして学習と評価をK回繰り返します。最終的にK回の評価結果の平均を取ります。 | - データの利用効率が高い。<br>- 評価結果の信頼性が高い。<br>- ホールドアウト法より計算コストが高い。 |

# 混合行列
![[Pasted image 20250903234246.png]]
TP (True Positive): 実際に陽性で、陽性と予測
FP (False Positive): 実際に陰性で、陽性と予測
FN (False Negative): 実際に陽性で、陰性と予測
TN (True Negative): 実際に陰性で、陰性と予測

# パーセプトロン
複数の入力を受け取り、それらを重み付けして合計し、活性化関数を通して出力を生成する、神経細胞（ニューロン）を模倣したアルゴリズム

| 種類        | 説明                                                    | 主な特徴                                                                                              |
| --------- | ----------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| 単一パーセプトロン | 入力層と出力層のみを持つ最もシンプルなモデル。線形分離可能な問題のみを解決できる。             | - 線形分類器。<br>- 論理和（OR）、論理積（AND）は学習できるが、排他的論理和（XOR）は学習できない。<br>- 学習には、誤差を最小化するパーセプトロンの学習規則が用いられる。   |
| 多層パーセプトロン | 入力層と出力層の間に、1つまたは複数の**隠れ層**を持つモデル。各層は複数のパーセプトロンで構成される。 | - 非線形な問題を解決できる。<br>- 隠れ層が複雑な特徴を学習することで、より複雑な関係をモデル化できる。<br>- 学習には、誤差逆伝播法（Backpropagation） が用いられる。 |

# ディープラーニング
ニューラルネットワークの隠れ層を増やしたもの。誤差関数を最小化するアプローチ
# 誤差関数

| 種類         | 説明                                | 主な特徴                                                                                   |
| ---------- | --------------------------------- | -------------------------------------------------------------------------------------- |
| 平均二乗誤差     | 予測値と正解値の差の2乗を平均したもの。              | - 回帰問題で最も一般的に使われる。<br>- 誤差が大きいほど、より大きなペナルティを与える。<br>- 外れ値に敏感。                          |
| 平均絶対誤差     | 予測値と正解値の差の絶対値を平均したもの。             | - 回帰問題に用いられる。<br>- MSEより外れ値に強い。<br>- 勾配が一定であるため、学習の収束が遅くなることがある。                       |
| 交差エントロピー誤差 | モデルの出力する確率分布と、正解の真の確率分布との間の乖離を測る。 | - 分類問題で最も広く使われる。<br>- モデルが間違った予測をした際に、大きなペナルティを与える。<br>- 多クラス分類では、カテゴリカル交差エントロピーが使われる。 |
# 過学習への対策

| 手法名     | 説明                                                     | 特徴                                                   |
| ------- | ------------------------------------------------------ | ---------------------------------------------------- |
| 正則化     | モデルの複雑さにペナルティを与えることで、過剰な重み（係数）の学習を抑制する手法。              | L1正則化やL2正則化が代表的。重みを小さく保つことで、モデルの汎化能力を高める。            |
| ドロップアウト | 訓練時に、ランダムにニューロンを無効化する手法。これにより、モデルが特定のニューロンに依存しすぎるのを防ぐ。 | 一種のアンサンブル学習のような効果を持ち、汎化性能が向上する。テスト時にはすべてのニューロンを使用する。 |
| 早期終了    | 訓練誤差が減少し続け、検証誤差が最小値に達したところで訓練を打ち切る手法。                  | 過学習が始まる前に訓練を止めることができる。単純で効果的な方法。                     |
| データ拡張   | 既存の訓練データを変形（回転、反転、拡大・縮小など）させることで、擬似的にデータ量を増やす手法。       | 画像認識の分野で特に有効。モデルが多様なデータパターンを学習できるようになり、汎化性能が向上する。    |
| バッチ正規化  | 訓練のミニバッチごとに、データの平均を0、標準偏差を1に正規化する手法。                   | 内部共変量シフトを軽減し、学習を高速化・安定化させる。過学習の抑制にも効果がある。            |
# パラメータ最適化手法
損失関数を最小化するためにモデルのパラメータ（重みやバイアス）を更新するアルゴリズム

|                       |                                                       |                                                                                  |
| --------------------- | ----------------------------------------------------- | -------------------------------------------------------------------------------- |
| 確率的勾配降下法 (SGD)オンライン学習 | 訓練データから**1つだけ**のサンプルをランダムに選び、その勾配を使ってパラメータを更新する。      | - BGDより高速。<br>- 学習の軌跡がノイジーで、局所的最小値に陥りにくい。<br>- 収束が遅く、安定しないことがある。                 |
| バッチ下降法（最急下降法、バッチ学習）   | 全ての訓練データを使って勾配を計算し、パラメータを一度だけ更新する。                    | - 非常に安定している。<br>- 計算コストが高く、大規模データには非効率的。<br>- 大域的最小値への収束が保証されている。                |
| ミニバッチ勾配下降法            | 訓練データから**ミニバッチ**と呼ばれるごく一部のサンプルを選び、その勾配を使ってパラメータを更新する。 | - 収束の安定性と計算コストのバランスが最も良い。<br>- 現在、最も広く使われている手法。<br>- ハイパーパラメータとしてミニバッチのサイズを調整する。 |
| モーメンタム                | SGDに「慣性」の概念を導入した手法。過去の勾配の移動平均を考慮して、現在の勾配の方向を加速させる。    | 谷のような平坦な場所で素早く収束する。振動を抑え、安定した学習を可能にする。                                           |
| AdaGrad               |                                                       |                                                                                  |

### 勾配下降法の問題
<font color="#ffff00">鞍点</font>に陥る可能性→こうした停滞状態を<font color="#ffff00">プラトー</font>
![[Pasted image 20250901162518.png]]
### 鞍点への対策

| 手法                | 説明                                                                                           |
| ----------------- | -------------------------------------------------------------------------------------------- |
| モーメンタム            | 最適化方向へ学習を加速させる方法                                                                             |
| Adagrad           | 頻繁に出現する特徴量には小さな学習率を、珍しい特徴量には大きな学習率を適用するが、学習が進むと<font color="#ffff00">学習率がゼロに収束してしまう欠点</font> |
| Adadelta          | AdaGradの欠点を改善した手法。過去の勾配の蓄積を抑えることで、学習率が極端に小さくなるのを防ぎ、学習率のハイパーパラメータを不要に                         |
| RMSprop           | AdaGradの問題を改善した手法で、勾配の二乗の移動平均を学習率の調整に用います。学習が安定し、収束が速い                                       |
| Adam              | RMSpropのアイデアに加えて、過去の勾配の慣性（勢い）も考慮に入れることで、高速かつ安定した学習                                           |
| AdaBound・AMSBound | Adamの学習率が発散する可能性を改善した手法。学習率に上限と下限を設けることで、学習の初期はAdamのように速く、終盤はSGDのように安定して収束する                 |
## 早期終了
誤差関数が右肩上がりになったときに学習を打ち切る
<font color="#ffff00">ノーフリーランチ定理</font>（<font color="#ffff00">あらゆる問題で性能の良い汎用最適化戦力は理論所不可能である</font>）を意識
ただ、<font color="#ffff00">二重降下現象</font>（1回誤差増えた後に下がる現象）もあるので学習を止めるタイミングは検討が必要
## ハイパーパラメータの最適化

| 項目        | 説明                                                                                  |
| --------- | ----------------------------------------------------------------------------------- |
| グリッドサーチ   | すべてのハイパーパラメータの組み合わせを、網羅的に試す手法。最も単純で確実だが、計算コストが非常に高い                                 |
| ランダムサーチ   | ランダムに選んだハイパーパラメータの組み合わせを試す手法。グリッドサーチよりも効率的に良い組み合わせを見つけられることが多い                      |
| ベイズ最適化    | これまでの試行結果から、次に試すべき最適なハイパーパラメータの組み合わせを予測し、効率的に探索する手法。計算コストは高いが、より少ない試行回数で最適な解を見つけやすい |
| 遺伝子アルゴリズム | 生物の進化を模倣した探索手法。複数の候補（遺伝子）をランダムに生成し、評価の高い候補を組み合わせて、より良い世代（組み合わせ）を作り出す。               |
## 誤差逆伝搬法
ニューラルネットワークの学習において、出力層で発生した誤差を逆向きに伝播させ、各層の重みとバイアスを効率的に更新するためのアルゴリズム
<font color="#ffff00">信用割合問題</font>（どのように予測結果を求めているかがわからない）を解決

誤差逆伝播法・・・<font color="#ffff00">どのように勾配を計算するか</font>
勾配降下法・・・<font color="#ffff00">勾配を使ってどのようにパラメータを更新するか</font>
### 誤差逆伝搬法の問題

| 問題     | 現象                                                                                     | 原因             |
| ------ | -------------------------------------------------------------------------------------- | -------------- |
| 勾配消失問題 | 勾配が層を遡るにつれてどんどん小さくなり、最終的にほぼゼロになってしまう現象<br>勾配がゼロになると、パラメータ（重み）の更新が行われなくなり、学習が進まなくなる<br> | シグモイド関数やtanh関数 |
| 勾配爆発問題 | 勾配が層を遡るにつれてどんどん大きくなり、最終的に非常に大きな値になってしまう現象                                              | 重みの初期値が大きすぎ    |
## 勾配消失問題への対策

・ReLU系活性化関数の利用
・Heの初期値やXavierの初期値など、適切な重みの初期値を設定
・バッチ正規化
### オートエンコーダ
## 勾配爆発問題への対策
重みの正則化

## 活性化関数
層の間をどのように伝搬させるか調整する関数
![[Pasted image 20250901170627.png]]



