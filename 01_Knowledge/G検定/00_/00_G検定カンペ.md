# AIの誕生

| 項目                                             | 説明                                                                       | 例                           |
| ---------------------------------------------- | ------------------------------------------------------------------------ | --------------------------- |
| レベル1<br><font color="#eeece1">ルールベースAI</font>  | すべての振る舞いがあらかじめ決められている<br>シンプルな制御システム                                     | エアコン、自動販売機                  |
| レベル2<br>古典的AI                                  | 探索・推論・知識データを用いて、<br>状況に応じた振る舞い                                           | 将棋やチェスのAI<br>エキスパートシステム     |
| レベル3<br><font color="#ffff00">機械学習AI</font>    | <font color="#eeece1">非常に多くのサンプルデータをもとに、<br>入力と出力の関係を学習するシステム</font><br> | レコメンデーションシステム<br>迷惑メールフィルター |
| レベル4<br><font color="#ffff00">ディープラーニング</font> | データから特徴量を自律的に学習する、<br>より高度なシステム                                          | 顔認証　音声認識　画像生成AI             |

| 年    | 内容                                                                                                                                                                                      |
| ---- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1946 | 世界初の汎用電子計算機<font color="#ffff00">エニアック(ENIAC) </font>誕生                                                                                                                                 |
| 1956 | ダートマス会議<br>①人工知能という言葉が<font color="#ffff00">ジョン・マッカーシー</font>によって初めて使用<br>②ハーバート・サイモン、アレン・ニューウェル、クリフ・ショーの数学の定理を自動的に証明する初期のAIプログラム<font color="#ffff00">ロジック・セオリスト</font> デモンストレーション<br> |

# 強いAIと弱いAIの提唱（ジョンサール）

| 項目   | 説明                    | 例        |
| ---- | --------------------- | -------- |
| 強いAI | 汎用的人工知能               | 自我を持ったAI |
| 弱いAI | 特化型人工知能<br>→中国語の部屋などで | AlphaGo  |

# 第一次AIブーム（1950年代後半〜1960年代）
<font color="#ffff00">トイ・プロブレム</font>（パズルや迷路探索といった限られた問題）で成果を上げる
現実世界の複雑な情報や未知の状況を扱うことができない<font color="#ffff00">フレーム問題</font>（1969）に直面

| 項目                                | 内容                                                                                                           |
| --------------------------------- | :----------------------------------------------------------------------------------------------------------- |
| ハノイの塔                             | ルール<br>1.  一度に一つの円盤しか動かせない。<br>2. 小さい円盤の上に大きい円盤を乗せてはいけない。<br>3. 3本ある杭のどれを使ってもよい。<br>最小手数で解くには、2n−1回の手数が必要となる |
| STRIPS                            | 前提・行動・結果を記述するプランニング                                                                                          |
| 1968～1970年<br>SHRDLU（テリー・ウィノグラード） | プランニングを「積み木の世界」で完全再現<br>自然言語だけで積み木を作れた                                                                       |
| 1984年<br>Cycプロジェクト（ダグ・レナート）       | 常識を記号の形で表現し、それを基にコンピュータが論理的な推論を行えるようにすることを目指す                                                                |


# ボードゲームパターン数

| ゲーム | パターン数  |
| --- | ------ |
| オセロ | 10^60  |
| チェス | 10^120 |
| 将棋  | 10^220 |
| 囲碁  | 10^360 |

# 探索アルゴリズム

|             |                                                                      | 図                                    |
| ----------- | -------------------------------------------------------------------- | ------------------------------------ |
| 幅優先探索       | 根から近いノードから順に探索<br>すべてのノードをキューに保持する必要がある<br>→メモリ不足になる                 | ![[Pasted image 20250901174929.png]] |
| 深さ優先探索      | 根からできるだけ深く探索<br>一度に一つの経路上のノードだけを記憶すればよい<br>→メモリ不足になりにくい<br>          | ![[Pasted image 20250901174941.png]] |
| Mini-Max法   | 相手も自分も最善を尽くすという前提で、ゲームのすべての可能な局面をツリー状に展開し、最終的な評価値が最大（ミニ）になるような手を選ぶ方法 | ![[Pasted image 20250901105639.png]] |
| Alpha-Beta法 | ミニマックス法を改良したもので、明らかに不利な局面の探索を途中で打ち切る                                 | ![[Pasted image 20250901105752.png]] |
| ヒューリスティック探索 | ヒューリスティクスを使って、より効率的に探索を進める方法                                         |                                      |
| モンテカルロ木探索   | ランダムな試行（シミュレーション）を繰り返して、最も有望な手を見つけ出します。                              |                                      |

## AI効果
かつて人工知能（AI）だと考えられていた技術が、普及して当たり前になると、もはやAIとはみなされなくなる現象
## 1950年：チューリングテスト（アラン・チューリング）
機械が人間と同等の知的振る舞いを示せるかを判定するためのテスト
## 1991年：ローブナーコンテスト（Loebner Prize） 
 チューリングテストの実践版として始まった年次コンテスト
## 1966年：イライザ（ELIZA） （ジョセフ・ワイゼンバウム） 人工無能
初期の対話プログラムです。キーワードに反応して質問を返すという単純な仕組み
イライザ効果：イライザを人間と勘違いする現象
## 身体性
知能獲得するには<font color="#ffff00">身体が不可欠</font>とする考え方
## 2045年：シンギュラリティ（レイ・カーツワイル）
AIが自らより優れたAIを開発し、そのAIがさらに優れたAIを開発するという「知能の爆発的連鎖」が起こることで、人類の知性を超える超知能（Superintelligence）が誕生するという考え

# 第二次AIブーム（1980年代〜）
知識獲得のボトルネック・シンボルグラウンディング問題
## エキスパートシステム
知識ベースと推論エンジンにより構成

| 名前      | 年    | 開発者            | 説明                                                                               |
| ------- | ---- | -------------- | -------------------------------------------------------------------------------- |
| DENDRAL | 1960 | エドワード・ファイゲンバウム | 質量スペクトルデータから、未知の有機化合物の分子構造を推定するシステム。専門家が利用する「ヒューリスティック（発見的手法）」をルール化して組み込んだ。      |
| MYCIN   | 1970 | スタンフォード大学      | 血液感染症の原因菌を特定し、治療薬を推奨するシステム。不確実な情報（発熱、症状など）から確率的に推論を行う「**確信度係数**」の概念を導入したことで知られる。 |
# オントロジー
意味ネットワークなどで用いられる知識の結び付け方の規則

| オントロジー種類       | 内容                                            |
| -------------- | --------------------------------------------- |
| ライトウェイト・オントロジー | 概念の分類（タクソノミー）や単純な語彙関係に焦点を当てた、比較的単純な構造のオントロジー。 |
| ヘビーウェイト・オントロジー | 複雑な概念間の関係性や制約、論理的な推論ルールを詳細に記述する、複雑な構造のオントロジー。 |

| 関連出来事      | 年    | 内容                                                                     |
| ---------- | ---- | ---------------------------------------------------------------------- |
| Cycプロジェクト  | 1984 | 人間のすべての一般常識をデータベース化し、知識ベースを構築することで人間と同等の推論システムを目指しました 。                |
| セマンティックウェブ |      | ウェブ上のデータをコンピュータが理解・解釈できるようにするための技術や概念の総称。オントロジーは、このセマンティックウェブの中核技術の一つ。 |
| ワトソン（IBM）  | 2011 | アメリカのクイズ番組ジョパディーで勝利(ライトオントロジーを使用)<br>質問に対して高速で検索しているだけ                 |
| 東ロボ君       | 2011 | 東京大学の合格を目指すも意味理解に苦しみ凍結                                                 |
# 第三次AIブーム（2000年代後半〜現在） 機械学習・特徴表現学習の時代

| 種類     | 内容            |
| ------ | ------------- |
| 教師あり学習 | ラベルを学習しラベルを予想 |
| 教師なし学習 | 構造やパターンを抽出    |
| 強化学習   | 将来の報酬を最大化     |
# 教師あり学習

## 回帰

| 項目          | 特徴                                                                             |
| ----------- | ------------------------------------------------------------------------------ |
| 線形回帰        | 入力特徴量と出力の間を、線形な関係でモデル化する最も基本的な手法。重み（係数）とバイアス（切片）を学習                            |
| 多項式回帰       | 線形回帰を拡張したもので、入力特徴量の多項式（2次、3次など）を使って非線形な関係をモデル化                                 |
| リッジ回帰       | 線形回帰に**L2正則化**（L2 regularization）を導入した手法。係数を小さくすることで、過学習を防ぎ、モデルの汎化能力を高める       |
| ラッソ回帰       | 線形回帰に**L1正則化**（L1 regularization）を導入した手法。重要でない特徴量の係数を強制的にゼロにすることで、特徴量選択を行う     |
| 決定木回帰       | データを階層的に分割し、各領域の平均値を出力として予測する手法。シンプルなルールベースのモデルを構築                             |
| ランダムフォレスト回帰 | 複数の決定木を組み合わせて予測を行う**アンサンブル学習**の一種。各決定木の予測の平均値を出力とする。                           |
| サポートベクター回帰  | サポートベクターマシン（SVM）を回帰に応用したもの。マージン内にできるだけ多くのデータポイントを収め、マージン外の誤差を最小化するようにモデルを学習する。 |


| 項目  | 特徴                                     |
| --- | -------------------------------------- |
| 単回帰 | 1つの独立変数（説明変数） を用いて、1つの従属変数（目的変数） を予測   |
| 重回帰 | 2つ以上の独立変数（説明変数） を用いて、1つの従属変数（目的変数） を予測 |

## 分類

| 項目          | 特徴                                                                                                                     |
| ----------- | ---------------------------------------------------------------------------------------------------------------------- |
| ロジスティック回帰   | 出力が0から1の間の確率であるため、分類問題によく用いられる。この確率をもとにクラスを判別する。                                                                       |
| サポートベクターマシン | マージン内にできるだけ多くのデータポイントを収め、マージン外の誤差を最小化するようにモデルを学習する。<br>**カーネルトリック**とそれを実現する**カーネル関数**によって、線形分離できない複雑なデータに対しても高い分類性能を発揮 |
| ランダムフォレスト   | 複数の決定木をランダムに作成し、それぞれの予測結果を多数決で最終的なクラスを決定する**アンサンブル学習**                                                                 |
| K-近傍法       | 予測したいデータに最も近い「K個」の訓練データを探し、それらのデータの多数決でクラスを決定する。                                                                       |
| ナイーブベイズ     | ベイズの定理に基づいて、各特徴量が独立であると仮定して確率を計算し、最も確率の高いクラスを予測する。                                                                     |
| ニューラルネットワーク | 脳の神経回路を模倣したモデル。入力層、中間層（隠れ層）、出力層から構成され、複雑な非線形関係を学習できる。                                                                  |

# アンサンブル学習

| 項目      | 特徴                                                                                           | 例                                     |
| ------- | -------------------------------------------------------------------------------------------- | ------------------------------------- |
| バギング    | 訓練データから**ブートストラップ法**（復元抽出）で複数のサブセットを作成し、それぞれでモデルを学習させる。最終的な予測は、**多数決（分類）** や**平均（回帰）** で決める。 | ランダムフォレスト                             |
| ブースティング | 性能の低いモデルを**逐次的**に学習させる。前のモデルが誤って予測したデータに重みを付けて、次のモデルがそのデータをより重視して学習する。                       | **AdaBoost**、**XGBoost**、**LightGBM** |
| スタッキング  | 複数の異なる種類のモデル（例：決定木、SVM）を学習させ、それらの予測結果を新たな特徴量として、**メタモデル**（最終的なモデル）を学習させる。                    |                                       |

| 項目       | 内容                                                                    | 特徴                                               |
| -------- | --------------------------------------------------------------------- | ------------------------------------------------ |
| AdaBoost | 誤分類されたデータに重みを加算し、次の学習器がそのデータをより重視して学習する。                              | 比較的速いが、XGBoostやLightGBMには劣る。                     |
| XGBoost  | **勾配ブースティング**の一種。損失関数の勾配を使ってモデルを逐次的に最適化する。過学習を防ぐための正則化機能を持つ。          | 並列処理に対応しているため、AdaBoostより速い。                      |
| LightGBM | **勾配ブースティング**の一種。XGBoostよりも高速で、大規模データに強い。葉っぱ単位で成長する**リーフワイズ**な決定木を使う。 | 非常に高速。**ヒストグラムベース**のアプローチにより、XGBoostより数倍速いことがある。 |

# 教師なし学習

## クラスタリング　データをグループ分けすること

| 項目          | 内容                                                                    |
| ----------- | --------------------------------------------------------------------- |
| 階層なしクラスタリング | クラスター数が固定されており、階層的な関係は作られません。アルゴリズムが高速で、大規模データに適しています。                |
| 階層ありクラスタリング | データポイント間の距離に基づいて、ツリー構造（デンドログラム）を構築します。クラスター数は後から決定できますが、計算コストが高くなります。 |

| 項目       | 内容                                                                                                   | 特徴                                                                  |
| -------- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| k-menas法 | **階層なしクラスタリング**の代表的な手法。事前に指定したクラスター数 (k) に基づき、データポイントを最も近いクラスター中心（重心）に割り当てていく。                       | **高速**でシンプル。大規模データに有効。クラスターの形状が**球状**であると仮定する。クラスター数を事前に決める必要がある。   |
| ウォード法    | **階層ありクラスタリング（凝集型）** の手法の一つ。クラスター間の距離を、**合併による情報損失**の増加量として定義する。クラスターの重心からの平方和を最小化するようにクラスターを併合していく。 | クラスターのサイズがほぼ均等になる傾向がある。**凝集型**の中で最も人気があり、バランスの取れたクラスターを形成する。        |
| 最短距離法    | **階層ありクラスタリング（凝集型）** の手法の一つ。2つのクラスター間の距離を、**それぞれのクラスターに属する最も近いデータポイント間の距離**と定義する。                    | **チェーン効果 (chaining effect)** が起こりやすく、細長いクラスターを形成しやすい。ノイズや外れ値に非常に敏感。 |
## 主成分分析（PCA）　データの次元を減らすこと

| 項目       | 内容                                                                        | 特徴                                                                                                                  |                                      |
| -------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------ |
| 特異値分解    | 行列を3つの行列（特異ベクトル、特異値）に分解する線形代数の手法。**主成分分析（PCA）** の実用的な計算方法として広く用いられる。      | - **線形**な手法。<br>- 元のデータの分散（バラつき）を最大化するように次元削減を行う。<br>- **情報圧縮**や**ノイズ除去**、**レコメンデーションシステム**に用いられる。                  |                                      |
| 多次元尺度構成法 | データ間の**距離や類似度**を保ちながら、高次元データを低次元空間にマッピングする手法。                             | データの**相対的な位置関係**を重視する。<br>- データそのものではなく、**データ間の距離行列**を入力として使用する。<br>- **心理学**や**マーケティング**分野で、消費者の好みなどを可視化するのに用いられる。 | ![[Pasted image 20250901142102.png]] |
| t-SNE    | 高次元空間でのデータ間の類似性を、低次元空間での確率分布として表現する**非線形**な次元削減手法。特に、**局所的な構造**を保つことに優れる。 | **非線形**な手法。<br>- クラスターやグループなどの**局所的な構造を非常に明確に可視化**できる。<br>- **データ可視化**に特化しており、機械学習の前処理としてはあまり使われない。                 | ![[Pasted image 20250901142317.png]] |

# 協調フィルタリング
多くのユーザーの行動や好みを利用して、新しいアイテムを推薦するレコメンデーション手法

| 項目               | 内容                                                         | 特徴                                                                           |
| ---------------- | ---------------------------------------------------------- | ---------------------------------------------------------------------------- |
| ユーザーベース協調フィルタリング | ターゲットユーザーと似た好みを持つ他のユーザーを見つけ出し、そのユーザーが良い評価をしたアイテムを推薦する手法です。 | 直感的で分かりやすい。ユーザー数が増加すると計算コストが急増する。新しいユーザーへの推薦が難しい「コールドスタート問題」に弱い。             |
| アイテムベース協調フィルタリング | ユーザーが過去に良い評価をしたアイテムと似たアイテムを推薦する手法です。アイテム間の類似度を計算する。        | ユーザーベースに比べて安定した推薦が可能。新しいアイテムが参入した際のコールドスタート問題に弱い。Netflixの推薦システムで使われたことで知られる。 |
| モデルベース協調フィルタリング  | データから潜在的なパターンや特徴を学習する機械学習モデルを構築し、それに基づいて推薦を行う手法です。         | スケーラビリティが高く、大規模データに対応できる。予測精度が高い。代表的な手法に行列分解がある。                             |
# トピックモデル
文書の集合（コーパス）の中から、潜在的な「トピック」を発見する教師なし学習の手法

| 項目          | 内容                                                                               |
| ----------- | -------------------------------------------------------------------------------- |
| 潜在的意味解析     | 文書と単語の共起行列に**特異値分解（SVD）** を適用することで、文書と単語を共通の潜在空間（トピック空間）に写像する手法です。               |
| 確率的潜在意味解析   | 文書がトピックの混合であり、各トピックが単語の確率分布であると仮定する**確率的モデル**です。文書と単語の共起を、トピックを介した確率としてモデル化します。  |
| 潜在的ディリクレ配分法 | **PLSA**を拡張した**ベイジアンモデル**です。各文書がトピックの確率分布の混合として生成され、各トピックが単語の確率分布として生成されると仮定します。 |
# 強化学習

| 項目           | 内容                                                                                                               |
| ------------ | ---------------------------------------------------------------------------------------------------------------- |
| バンディットアルゴリズム | 限られたリソースの中で、**探索（未知の選択肢を試す）** と**活用（既知の最良の選択肢を選ぶ）** のバランスを効率的にとるためのアルゴリズム                                        |
| ϵ-Greedy     | 確率εで**探索**を行い、確率1-εで**活用**を行うという、バンディット問題の解決策の一つ                                                                 |
| マルコフ決定過程     | 強化学習の多くの問題を数学的に定式化するためのフレームワーク<br><br>マルコフ性（Markov Property）<br>「ある時点の未来の状態は、現在の状態にのみ依存し、それまでの過去の履歴には依存しない」という性質 |

| 項目          | 内容                                                                                | 特徴と用途                                                                                                                                                                            |
| ----------- | --------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Q学習         | 価値ベース学習の代表。状態-行動ペアの**価値（Q値）** を学習し、そのQ値の最大化を目指す。行動選択には$\epsilon$-greedy法がよく用いられる。 | - オフポリシー（方策とは異なる行動から学習可能）。<br>- シンプルで実装が容易。<br>- 離散的な状態と行動空間を持つ問題に有効                                                                                                             |
| SARSA       | Q学習と同様の価値ベース学習。行動選択に用いた方策に基づいてQ値を更新する。                                            | - オンポリシー（行動に用いた方策から学習）。<br>- Q学習より安全な学習が期待される。                                                                                                                                   |
| DQN         | Q学習と**ディープラーニング**を組み合わせたもの。Q値をニューラルネットワークで近似することで、状態空間が膨大な問題にも対応可能。               | - **経験再生 (Experience Replay)**: 過去の経験をメモリに蓄え、そこからランダムにサンプリングして学習に用いる。これにより、データの相関が低減し、学習が安定する。 <br> - **ターゲットネットワーク**: 学習に用いるQ値の更新目標となるネットワークを、一定期間ごとに固定する。これにより、学習目標が安定し、発散を防ぐ。 |
| Double DQN  | **ターゲットQ値**の過大評価（overestimation）を抑制するために、Q値の計算方法を改良したDQN。                         | **Q値の過大評価を抑制**: 次の行動を評価する際に、オンラインネットワーク（最新のネットワーク）で行動を選び、ターゲットネットワークでその行動のQ値を評価する。これにより、過大評価が減り、安定した学習が可能になる。                                                                    |
| Dueling DQN | ネットワークのアーキテクチャを改良したDQN。                                                           | - **価値と優位性の分離**: ネットワークを「状態の価値 (V(s))」と「各行動の優位性 (Advantage, A(s,a))」に分離して学習する。これにより、各行動の価値をより正確に評価できる。                                                                           |
| Rainbow     | 上記の複数の改良点（DDQN, PER, Dueling DQN, C51など）を組み合わせたDQN。                               | - **高い性能**: それぞれの改良点の相乗効果により、単一のDQNよりも高い性能を発揮する。                                                                                                                                 |
| REINFORCE   | 方策勾配法（Policy Gradient Method）の代表。行動の価値ではなく、**行動方策（ポリシー）** を直接学習する。                | - 連続的な行動空間を持つ問題に適用可能。<br>- 方策ベース学習の最も基本的なアルゴリズム。                                                                                                                                 |
# モデルの評価

| 項目       | 内容                                                                                           | 特徴と用途                                                     |
| -------- | -------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| ホールドアウト法 | データを一度だけ訓練用とテスト用に分割し、テストデータでモデルを評価します。                                                       | シンプルで実装が容易。計算コストが低い。                                      |
| K-分割交差検証 | データをK個の等しいサイズのサブセットに分割します。そのうちの1つをテストデータ、残りのK−1個を訓練データとして学習と評価をK回繰り返します。最終的にK回の評価結果の平均を取ります。 | - データの利用効率が高い。<br>- 評価結果の信頼性が高い。<br>- ホールドアウト法より計算コストが高い。 |

# 混合行列
![[Pasted image 20250903234246.png]]
TP (True Positive): 実際に陽性で、陽性と予測
FP (False Positive): 実際に陰性で、陽性と予測
FN (False Negative): 実際に陽性で、陰性と予測
TN (True Negative): 実際に陰性で、陰性と予測

# パーセプトロン
複数の入力を受け取り、それらを重み付けして合計し、活性化関数を通して出力を生成する、神経細胞（ニューロン）を模倣したアルゴリズム

| 種類        | 説明                                                    | 主な特徴                                                                                              |
| --------- | ----------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| 単一パーセプトロン | 入力層と出力層のみを持つ最もシンプルなモデル。線形分離可能な問題のみを解決できる。             | - 線形分類器。<br>- 論理和（OR）、論理積（AND）は学習できるが、排他的論理和（XOR）は学習できない。<br>- 学習には、誤差を最小化するパーセプトロンの学習規則が用いられる。   |
| 多層パーセプトロン | 入力層と出力層の間に、1つまたは複数の**隠れ層**を持つモデル。各層は複数のパーセプトロンで構成される。 | - 非線形な問題を解決できる。<br>- 隠れ層が複雑な特徴を学習することで、より複雑な関係をモデル化できる。<br>- 学習には、誤差逆伝播法（Backpropagation） が用いられる。 |

# ディープラーニング
ニューラルネットワークの隠れ層を増やしたもの。誤差関数を最小化するアプローチ
# 誤差関数

| 種類         | 説明                                | 主な特徴                                                                                   |
| ---------- | --------------------------------- | -------------------------------------------------------------------------------------- |
| 平均二乗誤差     | 予測値と正解値の差の2乗を平均したもの。              | - 回帰問題で最も一般的に使われる。<br>- 誤差が大きいほど、より大きなペナルティを与える。<br>- 外れ値に敏感。                          |
| 平均絶対誤差     | 予測値と正解値の差の絶対値を平均したもの。             | - 回帰問題に用いられる。<br>- MSEより外れ値に強い。<br>- 勾配が一定であるため、学習の収束が遅くなることがある。                       |
| 交差エントロピー誤差 | モデルの出力する確率分布と、正解の真の確率分布との間の乖離を測る。 | - 分類問題で最も広く使われる。<br>- モデルが間違った予測をした際に、大きなペナルティを与える。<br>- 多クラス分類では、カテゴリカル交差エントロピーが使われる。 |
# 過学習への対策

| 手法名     | 説明                                                     | 特徴                                                   |
| ------- | ------------------------------------------------------ | ---------------------------------------------------- |
| 正則化     | モデルの複雑さにペナルティを与えることで、過剰な重み（係数）の学習を抑制する手法。              | L1正則化やL2正則化が代表的。重みを小さく保つことで、モデルの汎化能力を高める。            |
| ドロップアウト | 訓練時に、ランダムにニューロンを無効化する手法。これにより、モデルが特定のニューロンに依存しすぎるのを防ぐ。 | 一種のアンサンブル学習のような効果を持ち、汎化性能が向上する。テスト時にはすべてのニューロンを使用する。 |
| 早期終了    | 訓練誤差が減少し続け、検証誤差が最小値に達したところで訓練を打ち切る手法。                  | 過学習が始まる前に訓練を止めることができる。単純で効果的な方法。                     |
| データ拡張   | 既存の訓練データを変形（回転、反転、拡大・縮小など）させることで、擬似的にデータ量を増やす手法。       | 画像認識の分野で特に有効。モデルが多様なデータパターンを学習できるようになり、汎化性能が向上する。    |
| バッチ正規化  | 訓練のミニバッチごとに、データの平均を0、標準偏差を1に正規化する手法。                   | 内部共変量シフトを軽減し、学習を高速化・安定化させる。過学習の抑制にも効果がある。            |
# パラメータ最適化手法
損失関数を最小化するためにモデルのパラメータ（重みやバイアス）を更新するアルゴリズム

| アルゴリズム名               | 概要                                                                        | 主な特徴と利点                                                                          |
| --------------------- | ------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| 確率的勾配降下法 (SGD)オンライン学習 | 訓練データから**1つだけ**のサンプルをランダムに選び、その勾配を使ってパラメータを更新する。                          | - BGDより高速。<br>- 学習の軌跡がノイジーで、局所的最小値に陥りにくい。<br>- 収束が遅く、安定しないことがある。                 |
| バッチ下降法（最急下降法、バッチ学習）   | 全ての訓練データを使って勾配を計算し、パラメータを一度だけ更新する。                                        | - 非常に安定している。<br>- 計算コストが高く、大規模データには非効率的。<br>- 大域的最小値への収束が保証されている。                |
| ミニバッチ勾配下降法            | 訓練データから**ミニバッチ**と呼ばれるごく一部のサンプルを選び、その勾配を使ってパラメータを更新する。                     | - 収束の安定性と計算コストのバランスが最も良い。<br>- 現在、最も広く使われている手法。<br>- ハイパーパラメータとしてミニバッチのサイズを調整する。 |
| モーメンタム                | SGDに「慣性」の概念を導入した手法。過去の勾配の移動平均を考慮して、現在の勾配の方向を加速させる。                        | 谷のような平坦な場所で素早く収束する。振動を抑え、安定した学習を可能にする。                                           |
| AdaGrad               | 個々のパラメータごとに学習率を調整する手法。頻繁に出現するパラメータの学習率は小さく、あまり出現しないパラメータの学習率は大きく更新する。     | 疎なデータセット（まばらなデータ）に非常に有効。学習が進むにつれて学習率が減少し、勾配がゼロに近づきやすい。                           |
| Adadelta              | AdaGradの学習率が急激に減衰する問題を解決した手法。勾配の累積ではなく、過去の勾配の移動平均を用いる                     | AdaGradと異なり、**学習率の減衰が緩やか**。学習率のハイパーパラメータを必要としないのが大きな特徴。                          |
| RMSprop               | AdaGradの改良版として開発された手法。勾配の二乗の指数移動平均を用いて学習率を調整する。                           | AdaGradの欠点を克服し、**非凸最適化問題**にも適している。AdaDeltaと同様に、学習率の減衰が緩やか。                       |
| Adam                  | **RMSprop**と**モーメンタム**を組み合わせた手法。勾配の一次モーメント（平均）と二次モーメント（非中心分散）の指数移動平均を用いる。 | 非常に高性能で、**汎用性**が高い。多くのタスクでデフォルトのオプティマイザとして使われる。調整すべきハイパーパラメータが少ない。               |
| AdaBound・AMSBound     | Adamの欠点である、学習率が極端な値を取る可能性があることを改善した手法。学習率に下限と上限を設けて制御する。                  | 学習の初期段階ではAdaGradやAdamのように振る舞い、学習の後半ではSGDのように**安定**して収束する。                        |
# ハイパーパラメータの最適化

| 項目        | 説明                                                                                  | 主な特徴と利点                                                                                  |
| --------- | ----------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- |
| グリッドサーチ   | 事前に定めたハイパーパラメータの組み合わせ（グリッド）を網羅的に試し、最も性能の良い組み合わせを見つける手法。                             | - シンプルで網羅的。最適な組み合わせを必ず見つけられる。<br>- パラメータの数が増えると、組み合わせが爆発的に増え、計算コストが非常に高くなる。              |
| ランダムサーチ   | 事前に定めた範囲からハイパーパラメータの組み合わせを**ランダムにサンプリング**して試し、最も性能の良い組み合わせを見つける手法。                  | - グリッドサーチより効率的。<br>- 重要なハイパーパラメータの値域をより広く探索できる。<br>- グリッドサーチと同じ計算コストでも、より良い結果を得られることが多い。 |
| ベイズ最適化    | 過去の試行結果を基に、次に試すべき最適なハイパーパラメータの組み合わせを予測する手法。獲得関数（Acquisition Function）を用いて、効率的に探索する。 | - 最も効率的な手法。計算コストが高いモデルの最適化に特に有効。<br>- 過去の情報を活用するため、試行回数を減らせる。<br>- 複雑な数理モデルに基づいている。      |
| 遺伝子アルゴリズム | 生物の進化（遺伝、突然変異、選択）を模倣した手法。性能の良いハイパーパラメータの組み合わせを「遺伝子」として扱い、世代を繰り返しながらより良い「遺伝子」を探索する。  | - 大域的最適解を見つけやすい。<br>- 直感的でわかりやすい。<br>- ベイズ最適化より効率は劣るが、多様な解を探索できる。                        |
| Optuna    | ベイズ最適化を応用したハイパーパラメータ最適化フレームワーク。試行結果から次に最適なパラメータの組み合わせを自動で探索する。                      | - 自動化されており、実装が容易。<br>- 並列化にも対応しており、高速。<br>- 様々なモデルやタスクに適用可能。                             |
# 誤差逆伝搬法の問題

| 問題点    | 説明                                                                                            | 対策                                                                                                                                        |
| ------ | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| 勾配消失問題 | 誤差が逆伝播する際に勾配がどんどん小さくなり、入力層に近い層の重みがほとんど更新されなくなる問題。特に、シグモイド関数やtanh関数などの活性化関数を持つ深いネットワークで発生しやすい。 | **ReLU（Rectified Linear Unit）** などの活性化関数を使用する。ReLUは、勾配が0になることがないため、勾配消失を防ぎやすい。また、**残差接続（Residual Connections）** を持つネットワーク構造（例：ResNet）も有効。 |
| 勾配爆発問題 | 誤差が逆伝播する際に勾配が非常に大きくなり、重みが極端に大きな値に更新される問題。これにより、学習が不安定になり、発散することがある。                           | **勾配クリッピング（Gradient Clipping）** を用いる。これは、勾配のノルム（大きさ）が一定の閾値を超えた場合に、その勾配をスケーリングして小さくする手法。                                                   |
# 活性化関数

| 種類                           | 式                                  | 特徴                                                                               | 主な用途                                                   |
| ---------------------------- | ---------------------------------- | -------------------------------------------------------------------------------- | ------------------------------------------------------ |
| シグモイド関数                      | f(x)=frac11+e−x                    | - 出力値が0から1の範囲に収まる。 <br> - 勾配が0に近づくため、勾配消失問題を引き起こしやすい。                            | - 出力層での二値分類の確率推定。 <br> - 歴史的に初期のニューラルネットワークで使われた。      |
| tanh関数                       | f(x)=fracex−e−xex+e−x              | - 出力値が-1から1の範囲に収まる。 <br> - シグモイド関数より勾配消失問題が緩和される。                                | - 中間層でよく使われる。 <br> - データの平均を0に近づける効果がある。               |
| ReLU (Rectified Linear Unit) | f(x)=max(0,x)<br><br>              | - 入力が0以下の場合は0、0より大きい場合は入力値そのものを返す。 <br> - 勾配消失問題が起こりにくい。 <br> - 計算が非常に高速。        | - 最も広く使われている中間層の活性化関数。 <br> - 多くの深層学習モデルでデフォルトとして使われる。 |
| Leaky ReLU                   | f(x)=max(alphax,x) （alphaは小さな正の定数） | - ReLUの改良版。入力が0以下でもわずかに勾配を持つ。 <br> - 「**死んだReLU**」（学習中に勾配が0になってしまうニューロン）問題を解決する。 | - ReLUの代替として、勾配消失を防ぎつつ安定した学習が必要な場合。                    |
| Softmax関数                    | f(x_i)=fracex_isum_jex_j           | - 入力された値の合計が1になるように、それぞれの値を確率として出力する。                                            | - 出力層での**多クラス分類**。 <br> - 各クラスの確率を表現するのに適している。         |
![[Pasted image 20250901170627.png]]

# CNN
ディープラーニングの中で最も成功しているモデル
## 物体認識・画像分類タスクに使われるCNN

| 項目           | 年代    | 人物                 | 説明                                                                                                                               | 図                                    |
| ------------ | ----- | ------------------ | -------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------ |
| ネオコグニトロン     | 1979年 | 福島邦彦               | 階層的な特徴抽出と位置ずれ不変性<br>S細胞：特徴を抽出する役割<br>C細胞：S細胞層で抽出された特徴の位置のずれを吸収                                                                   | ![[Pasted image 20250901190743.png]] |
| LeNet        | 1998年 | ヤン・ルカン             | 誤差逆伝搬法を用いるCNNの原型ともいえる初期のモデル。畳み込み層、プーリング層、全結合層で構成される基本的な構造を確立し、手書き数字認識に成功しました。                                                    | ![[Pasted image 20250901191054.png]] |
| AlexNet      | 2012年 | アレックス・クリジェフスキー     | 大規模なデータセット（ImageNet）で高い精度を達成し、CNNブームの火付け役となったモデル。GPUの活用、ReLU活性化関数、ドロップアウトを導入しました。<br><br>                                        | ![[Pasted image 20250901191937.png]] |
| GoogleNet    | 2014年 | Google             | **Inceptionモジュール**を導入し、複数のサイズの畳み込みフィルタを並列に適用する構造を特徴とします。これにより、計算コストを抑えつつ、モデルの幅を広げました。                                            | ![[Pasted image 20250901192258.png]] |
| VGG          | 2014年 | オックスフォード大学         | 3x3の小さなフィルタを複数積み重ねることで、表現力を高めたモデル。深いネットワークの構築における**シンプルな構造の重要性**を示しました。                                                          | ![[Pasted image 20250901192319.png]] |
| ResNet       | 2015年 | Microsoft Research | **残差接続（Residual Connection）** を導入し、深いネットワークでの勾配消失問題を解決しました。これにより、数百層の超深層ネットワークの学習が可能になりました。<br>                                  | ![[Pasted image 20250901192330.png]] |
| Wide ResNet  | 2016年 | セルゲイ・ザハロフ          | ResNetの残差ブロックの幅（チャネル数）を広げることで、パラメータ数を大幅に増やさずに性能を向上                                                                               | ![[Pasted image 20250901192656.png]] |
| DenseNet     | 2017年 | Gao Huang          | 各層をそれ以前のすべての層に接続する**密な接続**を特徴とします。これにより、勾配消失問題をさらに緩和し、パラメータ数を削減しました。<br>                                                         | ![[Pasted image 20250901192727.png]] |
| SENET        | 2017年 | Momenta            | ILSVRC優勝<br>SE（Squeeze-and-Excitation）ブロックを導入<br><font color="#ffff00">チャネル方向の情報を動的に重み付け</font>することで、重要な特徴を強調し、精度を向上             | ![[Pasted image 20250901192834.png]] |
| MobileNet    | 2017年 | Google             | <font color="#ffff00">Depthwise Separable Convolution</font>を導入し、計算量を大幅に削減。モバイルデバイスや組み込みシステムでの利用に適した軽量なモデル。                      | ![[Pasted image 20250901193514.png]] |
| NASNet       | 2018年 | Google             | Neural Architecture Search（NAS）を用いて、強化学習によって最適なネットワーク構造を自動で探索した最初のモデル                                                            | ![[Pasted image 20250901193631.png]] |
| MnasNet      | 2018年 | Google             | NASNetを改良し、モバイルデバイスのレイテンシ（遅延）を考慮してネットワーク構造を探索。実用的なパフォーマンスと効率を両立した。                                                               |                                      |
| EfficientNet | 2019年 | Google             | 深さ（depth）、幅（width）、解像度（resolution）の3つの次元をバランス良くスケーリングする<font color="#ffff00">複合スケーリング</font>を提案<br>パラメータ数が少ないけど精度がいい<br>→転移学習に有用 | ![[Pasted image 20250901193745.png]] |
## 物体検出タスクに使われるCNN
### 2段階アプローチ
①画像から物体が存在しそうな領域（領域候補）を抽出する。
②抽出された各領域をCNNで分類・位置調整する。
特徴・・・高精度だが、処理速度は遅め。
### 1段階アプローチ
画像全体を一度に処理し、物体の位置とクラスを同時に予測する。
特徴:・・・高速だが、精度は2段階アプローチに劣ることがあった

| 項目                                  | 年代    | 人物            | 説明                                                                                                                                                              | アプローチ    |
| ----------------------------------- | ----- | ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------- |
| R-CNN                               | 2014年 | Ross Girshick | 領域提案 (Region Proposal) とCNNによる特徴抽出を組み合わせた最初の成功モデル<br>画像から物体候補領域を2,000個抽出(Selective Search)し、各領域をCNNで分類することで、高い精度を実現したが、<font color="#ffff00">処理速度が遅いのが課題</font> | 2段階アプローチ |
| Fast R-CNN                          | 2015年 | Ross Girshick | R-CNNの改良版。画像全体に対して一度だけCNNを適用し、その特徴マップから各領域の特徴を抽出することで、処理速度を大幅に向上<br><font color="#ffff00">FPN</font>(Feature Pyramid Network)がこのモデルに組み込まれ、様々なスケールの物体検出精度が向上     | 2段階アプローチ |
| YOLO (You Only Look Once)           | 2016年 | Joseph Redmon | 1回の推論で、画像のグリッドセルごとに物体の位置とクラスを同時に予測<br>従来の2段階アプローチ（領域提案→分類）と異なり、<font color="#ffff00">非常に高速な処理が可能</font>                                                         | 1段階アプローチ |
| SSD (Single Shot MultiBox Detector) | 2016年 | Wei Liu       | YOLOと同様に1段階のモデルですが、異なるスケールの特徴マップを利用することで、様々なサイズの物体を効率的に検出します。                                                                                                   | 1段階アプローチ |
| Mask R-CNN                          | 2017年 | Kaiming He    | Faster R-CNNを拡張し、物体検出に加えてセグメンテーション（物体の輪郭をピクセル単位で識別）のタスクも同時に実行できるようにしました。                                                                                        | 2段階アプローチ |
| RetinaNet                           | 2017年 | Tsung-Yi Lin  | Focal Lossを導入し、前景（物体）と背景（物体でない）のアンバランスなデータ分布による学習の課題を解決しました。これにより、1段階の検出器でも高い精度を達成                                                                              | 1段階アプローチ |
| EfficientDet                        | 2019年 | Mingxing Tan  | EfficientNetをバックボーンに利用し、<font color="#ffff00">BiFPN (Bi-directional Feature Pyramid Network)</font>で特徴を融合。高い精度と効率性を両立させ、最先端のモデルとなりました。                          | 1段階アプローチ |
## セグメンテーションタスクに使用されるCNN
### セグメンテーションタスクの種類

| タスク名        | 概要                                         | 目的と出力                                                               | 代表的なモデル                                                   |
| ----------- | ------------------------------------------ | ------------------------------------------------------------------- | --------------------------------------------------------- |
| セマンティック     | 画像内の全てのピクセルを<br>事前に定義されたカテゴリ（例: 車、道路、空）に分類 | 画像内の各ピクセルがどのカテゴリに属するかを識別<br>同じカテゴリの物体は区別しない                         | FCN (Fully Convolutional Network)<br>U-Net<br>DeepLabシリーズ |
| インスタンス      | 同じカテゴリに属する個々の物体を区別しながら、ピクセル単位で分類           | 同じカテゴリ内の各インスタンス（個体）を異なる色などで識別し、個々の物体の正確な輪郭を特定                       | Mask R-CNN                                                |
| パノプティック<br> | セマンティック＋インスタンス                             | 数えられる物体、例: 人、車と<br>数えられない領域、例: 空、道路<br>の両方を識別<br>画像全体を統一的にセグメンテーション | Panoptic FPN<br>UPSNet                                    |
![[Pasted image 20250901195854.png]]

| モデル名    | 主な特徴と技術                                                                                                                                                                      |
| ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| U-Net   | エンコーダー・デコーダー構造とスキップ接続が特徴<br>エンコーダーで画像を圧縮して高次特徴を抽出し<br>デコーダーで空間的な情報を復元<br><br>スキップ接続により、高解像度の特徴を直接デコーダーに伝達し、<br>細かい物体の輪郭を正確に捉えることを可能に                                         |
| PSPNet  | Pyramid Scene Parsingという手法を導入<br>画像全体を複数の異なるスケールでプールし、その結果を統合することで、文脈情報を豊富に捉える<br><br>これにより、シーン全体を理解し、画像内の物体だけでなく、<br>その背景や文脈も考慮した高精度なセグメンテーションを実現                           |
| DeepLab | アトラス畳み込み（Atrous Convolution）とAtrous Spatial Pyramid Pooling (ASPP)を導入<br><br>アトラス畳み込みにより、プーリングを使わずに受容野を広げることができ、空間解像度の低下を防ぐ<br>ASPPは、複数の異なるアトラスレートで畳み込みを行い、様々なスケールの特徴を抽出<br> |

## 姿勢推定タスクに使用されるCNN

| モデル名     | 登場年   | 主な特徴と技術登場年                                                                                                      |
| -------- | ----- | --------------------------------------------------------------------------------------------------------------- |
| OpenPose | 2017年 | 1枚の画像から、複数の人物の姿勢を同時に推定<br>関節点を示すヒートマップと、関節点間の接続を示す<br><font color="#ffff00">部位接続（PAFs）</font>を同時に予測する二つのブランチを持つ |
| HRNet    | 2019年 | ネットワーク全体を通して高解像度の特徴表現を維持<br>異なる解像度の並列ストリーム間で情報を繰り返し交換し、<br>高精度のキーポイントヒートマップを生成                                  |
## データ拡張


