# 自然言語処理（Natural Language Processing: NLP）
人間が日常的に使っている言葉をコンピュータに理解・生成させるための技術
## データの扱い方
コンピュータは言葉を直接理解できないため、単語を数値データに変換する必要がある
### N-gram
文章をN個の単語のまとまりとして捉える手法
例：「私は猫が好き」という文を2-gramにすると「私は猫」「猫が好き」となります。
### Bag-of-Words (BoW)
文中の単語を数え、単語の出現頻度だけを考慮してベクトル化する手法
単語の順番や文脈は無視
![[Pasted image 20250901220956.png]]
### TF-IDF
BoWを改良した手法で、特定の文書に特有の重要な単語ほど重みを大きくする手法
![[Pasted image 20250901221019.png]]
# 単語の数値表現 
## 局所表現（Local Representation）
単語を個別のIDとして扱う手法
例：ワンホットベクトル（One-Hot Vector）がその代表で、単語を「1」が1つだけ存在する非常に長いベクトルで表現
## 分散表現（Distributed Representation）
単語を、多次元の密なベクトルで表現する手法
主成分分析を用いて2次元に次元圧縮する
単語埋め込み（Word Embedding）
意味的に似た単語は、ベクトル空間で近い位置に配置される
→単語間の関係性をコンピュータが理解できるようになる
## Word2Vec
単語埋め込みを学習するための代表的なモデル
<font color="#ffff00">CBOW (Continuous Bag-of-Words)</font>・・・周辺の単語から中心の単語を予測
<font color="#ffff00">Skip-gram</font>・・・中心にある単語から周辺の単語を予測
### fastText
Word2Vecを改良したモデルで活用形まで使用できる
# 事前学習モデルの進化 
## ELMo (Embeddings from Language Models)
文脈を考慮した単語埋め込みを生成するモデル
同じ単語でも、文脈によって異なるベクトルを生成します。
## BERT (Bidirectional Encoder Representations from Transformers)
Googleが開発したモデルで、文章の双方向の文脈を同時に考慮して学習
これにより、単語の意味をより深く理解できるようになり、
多くのNLPタスクで<font color="#ffff00">SOTA（State-of-the-Art）</font>を達成
## 事前学習モデル (Pre-trained Models)
大量のテキストデータで事前に学習されたモデル
特定のタスクに微調整（ファインチューニング）するだけで、高い性能を発揮
## GPT (Generative Pre-trained Transformer)
OpenAIが開発したモデルで、Transformerのデコーダー部分を使い、文章生成に特化
GPT-2、GPT-3と進化を続け、より自然で人間らしい文章を生成できるようになった
## PaLM (Pathways Language Model)
Googleが開発した大規模言語モデル（LLM）で、非常に多くのパラメータを持ち、高い推論能力を誇る