# 自然言語処理（Natural Language Processing: NLP）
人間が日常的に使っている言葉をコンピュータに理解・生成させるための技術
## データの扱い方
コンピュータは言葉を直接理解できないため、単語を数値データに変換する必要がある
### N-gram
文章をN個の単語のまとまりとして捉える手法
例：「私は猫が好き」という文を2-gramにすると「私は猫」「猫が好き」となります。
### Bag-of-Words (BoW)
文中の単語を数え、単語の出現頻度だけを考慮してベクトル化する手法
単語の順番や文脈は無視
![[Pasted image 20250901220956.png]]
### TF-IDF
BoWを改良した手法で、特定の文書に特有の重要な単語ほど重みを大きくする手法
![[Pasted image 20250901221019.png]]
# 単語の数値表現 
## 局所表現（Local Representation）
単語を個別のIDとして扱う手法
例：ワンホットベクトル（One-Hot Vector）がその代表で、単語を「1」が1つだけ存在する非常に長いベクトルで表現
## 分散表現（Distributed Representation）
単語を、多次元の密なベクトルで表現する手法。単語埋め込み（Word Embedding）
意味的に似た単語は、ベクトル空間で近い位置に配置される
→単語間の関係性をコンピュータが理解できるようになる
## Word2Vec
単語の分散表現を学習するモデルの代表
<font color="#ffff00">CBOW (Continuous Bag-of-Words)</font>・・・周辺の単語から中心の単語を予測
<font color="#ffff00">Skip-gram</font>・・・中心にある単語から周辺の単語を予測
### fastText
Word2Vecを改良したモデルで活用形まで使用できる
# 事前学習モデルの進化 
ELMo (Embeddings from Language Models): 文脈を考慮した単語埋め込みを生成するモデルです。同じ単語でも、文脈によって異なるベクトルを生成します。

BERT (Bidirectional Encoder Representations from Transformers): Googleが開発したモデルで、文章の双方向の文脈を同時に考慮して学習します。これにより、単語の意味をより深く理解できるようになり、多くのNLPタスクでSOTA（State-of-the-Art）を達成しました。

事前学習モデル (Pre-trained Models): 大量のテキストデータで事前に学習されたモデルのことです。このモデルは、特定のタスク（例：感情分析）に微調整（ファインチューニング）するだけで、高い性能を発揮します。Transformerモデルの登場により、この手法が主流となりました。

GPT (Generative Pre-trained Transformer): OpenAIが開発したモデルで、Transformerのデコーダー部分を使い、文章生成に特化しています。GPT-2、GPT-3と進化を続け、より自然で人間らしい文章を生成できるようになりました。

ChatGPT: GPTを会話に特化させてファインチューニングしたモデルです。

PaLM (Pathways Language Model): Googleが開発した大規模言語モデル（LLM）で、非常に多くのパラメータを持ち、高い推論能力を誇ります。

NLPの応用タスク 🗣️
機械翻訳: ある言語から別の言語に文章を自動的に変換するタスクです。Seq2seq（Encoder-Decoder）モデルが基礎となり、AttentionとTransformerの導入で性能が飛躍的に向上しました。

構文解析: 文の構造（主語、述語、目的語など）を分析するタスクです。

形態素解析: 文章を意味を持つ最小単位（形態素）に分解するタスクです。日本語では、この解析が不可欠です。

感情分析: 文章がポジティブかネガティブか、といった感情を分析するタスクです。

情報検索: データベースやウェブから、関連性の高い情報を探し出すタスクです。

質問応答: 質問に対して、文章やデータベースから適切な回答を生成するタスクです。

文書要約: 長い文書を要約して、簡潔な文章を生成するタスクです。