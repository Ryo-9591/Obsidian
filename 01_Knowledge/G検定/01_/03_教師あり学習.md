# 線形回帰
## 単回帰
1つの説明変数を使って、1つの目的変数を予測する手法
## 重回帰
2つ以上の説明変数を使って、1つの目的変数を予測する手法
![[Pasted image 20250901130153.png]]
## ラッソ回帰
過学習を防ぐために、目的関数に<font color="#ffff00">L1正則化項</font>(モデルの重みの<font color="#ffff00">絶対値</font>の合計に比例するペナルティ)を追加します。
重要でない重みを<font color="#ffff00">完全にゼロにする</font>ことができる
![[Pasted image 20250901130751.png]]
## リッジ回帰
過学習を防ぐために、目的関数に<font color="#ffff00">L2正則化項</font>(モデルの重みの<font color="#ffff00">2乗</font>の合計に比例するペナルティ)を追加します。
これにより、重みをゼロに近づけますが、<font color="#ffff00">完全にゼロにはしません</font>。
![[Pasted image 20250901132359.png]]

# 分類問題
## ロジスティクス回帰
### シグモイド関数
2クラス分類が得意
![[Pasted image 20250901132935.png]]
### ソフトマックス関数
多クラス分類が得意
![[Pasted image 20250901133226.png]]
### ランダムフォレスト
それぞれの決定木に対してランダムに一部取り出して学習を取り出すのを<font color="#ffff00">ブーストラップサンプリング</font>
![[Pasted image 20250901133543.png]]

---
# アンサンブル学習
## バギング
複数のモデルを独立して学習させ、それらの予測結果を統合
## ブースティング
複数の弱い学習器（性能の低いモデル）を直列につなぎ、前のモデルが間違えたデータに重みを付けて学習

AbaBoost
勾配ブースティング
XGBoost