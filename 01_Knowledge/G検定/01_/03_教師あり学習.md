# 線形回帰
## 単回帰
1つの説明変数を使って、1つの目的変数を予測する手法
## 重回帰
2つ以上の説明変数を使って、1つの目的変数を予測する手法
![[Pasted image 20250901130153.png]]
## ラッソ回帰
過学習を防ぐために、目的関数に<font color="#ffff00">L1正則化項</font>(モデルの重みの<font color="#ffff00">絶対値</font>の合計に比例するペナルティ)を追加します。
重要でない重みを<font color="#ffff00">完全にゼロにする</font>ことができる
![[Pasted image 20250901130751.png]]
## リッジ回帰
過学習を防ぐために、目的関数に<font color="#ffff00">L2正則化項</font>(モデルの重みの<font color="#ffff00">2乗</font>の合計に比例するペナルティ)を追加します。
これにより、重みをゼロに近づけますが、<font color="#ffff00">完全にゼロにはしません</font>。
![[Pasted image 20250901132359.png]]

# 分類問題
## ロジスティクス回帰
### シグモイド関数
2クラス分類が得意
![[Pasted image 20250901132935.png]]
### ソフトマックス関数
多クラス分類が得意
![[Pasted image 20250901133226.png]]
### ランダムフォレスト
それぞれの決定木に対してランダムに一部取り出して学習を取り出すのを<font color="#ffff00">ブーストラップサンプリング</font>
![[Pasted image 20250901133543.png]]

---
# アンサンブル学習
## バギング
複数のモデルを独立して学習させ、それらの予測結果を統合
## ブースティング
複数の弱い学習器（性能の低いモデル）を直列につなぎ、前のモデルが間違えたデータに重みを付けて学習（<font color="#ffff00">AdaBoost</font>）
バギングより逐次的に学習を進めるので時間がかかる
### 勾配ブースティング (Gradient Boosting)
AdaBoostとは異なる方法で弱点を克服します。
<font color="#ffff00">予測誤差</font>（残差）を最小化するように、次のモデルを学習させます。
これは、<font color="#ffff00">勾配降下法を応用したアプローチ</font>です。
### XGBoost (eXtreme Gradient Boosting)
XGBoostは、勾配ブースティングをさらに進化させた、高性能なアルゴリズムです。
その最大の強みは速度と精度
- **高速性**: 効率的なデータ探索と並列処理によって、学習速度が非常に速い。
- **高精度**: 過学習を防ぐための**L1/L2正則化**を導入しており、高い予測精度を誇る。
---
# サポートベクターマシン（SVM）
**サポートベクターマシン（SVM）**は、データを最も効率的に分ける**決定境界**を見つけることで、分類や回帰を行う教師あり学習モデルです。異なるクラスのデータ点を分離する際に、決定境界と、それに最も近いデータ点（**サポートベクター**）との間の距離（**マージン**）を最大化することを目指します。これにより、過学習を防ぎ、未知のデータに対する高い予測精度を実現します。

## 高次元データとカーネル法

SVMは、もともと2つのクラスを線形（直線）で分類するモデルですが、現実のデータは必ずしも直線で分けられるわけではありません。このような**線形分離できない**データを扱うため、SVMは**カーネル法**という手法を用います。

### カーネル法

カーネル法は、データをより高次元の空間にマッピングすることで、低次元では分離不可能だったデータが線形で分離できるようにします。

### カーネルトリック

高次元空間での計算は非常に複雑になりますが、SVMは**カーネルトリック**を用いることで、実際に高次元にマッピングすることなく、**元の低次元空間での計算**によって、高次元空間での内積を効率的に求めることができます。これにより、計算が複雑になることなく、非線形なデータにも対応できる柔軟性を獲得します。