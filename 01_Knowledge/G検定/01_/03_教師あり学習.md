# 線形回帰
## 単回帰
1つの説明変数を使って、1つの目的変数を予測する手法
## 重回帰
2つ以上の説明変数を使って、1つの目的変数を予測する手法
![[Pasted image 20250901130153.png]]
## ラッソ回帰
過学習を防ぐために、目的関数に<font color="#ffff00">L1正則化項</font>(モデルの重みの<font color="#ffff00">絶対値</font>の合計に比例するペナルティ)を追加します。
重要でない重みを<font color="#ffff00">完全にゼロにする</font>ことができる
![[Pasted image 20250901130751.png]]
## リッジ回帰
過学習を防ぐために、目的関数に<font color="#ffff00">L2正則化項</font>(モデルの重みの<font color="#ffff00">2乗</font>の合計に比例するペナルティ)を追加します。
これにより、重みをゼロに近づけますが、<font color="#ffff00">完全にゼロにはしません</font>。
![[Pasted image 20250901132359.png]]

# 分類問題
## ロジスティクス回帰
### シグモイド関数
2クラス分類が得意
![[Pasted image 20250901132935.png]]
### ソフトマックス関数
多クラス分類が得意
![[Pasted image 20250901133226.png]]
### ランダムフォレスト