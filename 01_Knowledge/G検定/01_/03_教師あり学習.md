# 線形回帰
![[Pasted image 20250901130153.png]]
## 単回帰
1つの説明変数を使って、1つの目的変数を予測する手法
## 重回帰
2つ以上の説明変数を使って、1つの目的変数を予測する手法

# 分類問題
## ロジスティクス回帰
### シグモイド関数
2クラス分類が得意
![[Pasted image 20250901132935.png]]
### ソフトマックス関数
多クラス分類が得意
![[Pasted image 20250901133226.png]]
### ランダムフォレスト
それぞれの決定木に対してランダムに一部取り出して学習を取り出すのを<font color="#ffff00">ブーストラップサンプリング</font>
![[Pasted image 20250901133543.png]]

---
# アンサンブル学習
## バギング
複数のモデルを独立して学習させ、それらの予測結果を統合
## ブースティング
複数の弱い学習器（性能の低いモデル）を直列につなぎ、前のモデルが間違えたデータに重みを付けて学習（<font color="#ffff00">AdaBoost</font>）
バギングより逐次的に学習を進めるので時間がかかる
### 勾配ブースティング (Gradient Boosting)
AdaBoostとは異なる方法で弱点を克服します。
<font color="#ffff00">予測誤差</font>（残差）を最小化するように、次のモデルを学習させます。
これは、<font color="#ffff00">勾配降下法を応用したアプローチ</font>です。
### XGBoost (eXtreme Gradient Boosting)
XGBoostは、勾配ブースティングをさらに進化させた、高性能なアルゴリズムです。
その最大の強みは速度と精度
- **高速性**: 効率的なデータ探索と並列処理によって、学習速度が非常に速い。
- **高精度**: 過学習を防ぐための**L1/L2正則化**を導入しており、高い予測精度を誇る。
---
# サポートベクターマシン（SVM）
・データを最も効率的に分ける決定境界を見つけることで、分類や回帰を行う教師あり学習モデル
・異なるクラスのデータ点を分離する際に、決定境界と、それに最も近いデータ点（**サポートベクター**）との間の距離（**マージン**）を最大化することを目指す
・過学習を防ぎ、未知のデータに対する高い予測精度を実現します。
・線形分離できないデータを扱うため、SVMはカーネル法という手法を用います。
### カーネル関数
データをより高次元の空間にマッピングすることで、低次元では分離不可能だったデータが線形で分離できるようにします。
### カーネルトリック
高次元空間での計算は非常に複雑になりますが、SVMは**カーネルトリック**を用いることで、実際に高次元にマッピングすることなく、**元の低次元空間での計算**によって、高次元空間での内積を効率的に求める

---
# 自己回帰モデル
<font color="#ffff00">時系列データ分析</font>で使われるモデルで、過去のデータを使って未来の値を予測する手法
## ベクトル自己回帰モデル（VARモデル）
<font color="#ffff00">複数の時系列データ</font>間の相互依存関係を分析・予測するための統計モデル

# オートエンコーダ
