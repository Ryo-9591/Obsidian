# RNN
隠れ層にリカレント結合を持ち、前の時刻の隠れ層の状態（情報）が、次の時刻の隠れ層の入力として渡される
→<font color="#ffff00">単語の並びや音声</font>、株価の推移といった時間的な文脈を持つデータを学習することができる
## RNNが直前の隠れ層の状態しか記憶できないという課題を克服

| 項目          | 特徴                                       | 図                                    |
| ----------- | ---------------------------------------- | ------------------------------------ |
| エルマンネットワーク  | 直前の隠れ状態を次の層の入力に<br><br>直前の文脈を考慮するタスク     | ![[Pasted image 20250901202322.png]] |
| ジョルダンネットワーク | 直前の最終的な出力を次の層の入力に<br><br>過去の行動履歴を考慮するタスク | ![[Pasted image 20250901202339.png]] |

## 勾配消失問題を克服

| モデル名                                 | 特徴                                                                   | 構造とゲート                          | メリット/デメリット                                                                    |
| ------------------------------------ | -------------------------------------------------------------------- | ------------------------------- | ----------------------------------------------------------------------------- |
| **LSTM**<br>(Long Short-Term Memory) | <font color="#ffff00">セル状態(CEC)</font>を持つ複雑なゲート構造で、<br>長期的な依存関係を制御する | 忘却ゲート<br>入力ゲート<br>出力ゲート<br><br> | メリット：<br>勾配消失を防ぎ、長期の時系列データで高い性能を発揮<br><br>デメリット：<br>パラメータ数が多く、計算コストが高い        |
| GRU<br>(Gated Recurrent Unit)        | LSTMを簡略化したモデル<br><br>セル状態と隠れ状態を統合                                    | 更新ゲート<br>リセットゲート<br><br>        | メリット：<br>LSTMに比べ、パラメータが少なく、高速に学習できる<br><br>デメリット：<br>タスクによってはLSTMより精度が劣る場合がある |
## BiRNN（双方向リカレントニューラルネットワーク）
RNNの<font color="#ffff00">未来の文脈情報が欠落するという課題</font>を解決
→順方向と逆方向の2つのRNNを同時に動かす

・順方向RNNは、過去の情報を捉る
・逆方向RNNは、未来の情報を捉る

これらの情報を組み合わせて利用することで、BiRNNは現在の入力に対して、過去と未来の両方の文脈を考慮した、より正確な理解と予測が可能に
## エンコーダーデコーダー
時系列データを時系列データとして出力するため
## sequence-to-sequence（Seq2seq）
エンコーダーとデコーダーという2つの部分から構成

| エンコーダー                                                                      | デコーダー                                                             |
| --------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| 入力されたシーケンス（例：日本語の文章「私はロボットです」）を読み込み、<br>その内容を圧縮してコンテキストベクトルと呼ばれる固定長のベクトルに変換 | エンコーダーから受け取ったコンテキストベクトルを基に、<br>新しいシーケンス（例：英語の文章「I am a robot」）を生成 |

### アテンション機構による進化
初期のSeq2seqモデル：すべての情報を一つのコンテキストベクトルに押し込めていたため、長いシーケンスを扱うと情報が失われる<font color="#ffff00">ボトルネック</font>という課題
### 解決したのがアテンション（Attention）機構
デコーダーが各単語を生成する際に、入力されたすべての単語のベクトルを参照し、最も関連性の高い単語に注目
