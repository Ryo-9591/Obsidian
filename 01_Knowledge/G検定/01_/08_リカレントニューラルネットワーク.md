# RNN
隠れ層にリカレント結合を持ち、前の時刻の隠れ層の状態（情報）が、次の時刻の隠れ層の入力として渡される
→<font color="#ffff00">単語の並びや音声</font>、株価の推移といった時間的な文脈を持つデータを学習することができる
## RNNが直前の隠れ層の状態しか記憶できないという課題を克服

| 項目          | 特徴                                       | 図                                    |
| ----------- | ---------------------------------------- | ------------------------------------ |
| エルマンネットワーク  | 直前の隠れ状態を次の層の入力に<br><br>直前の文脈を考慮するタスク     | ![[Pasted image 20250901202322.png]] |
| ジョルダンネットワーク | 直前の最終的な出力を次の層の入力に<br><br>過去の行動履歴を考慮するタスク | ![[Pasted image 20250901202339.png]] |

## 勾配消失問題を克服

| モデル名                                 | 特徴                                                     | 構造とゲート                                                | メリット/デメリット                                                                          |
| ------------------------------------ | ------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------------------------------------------------------- |
| **LSTM**<br>(Long Short-Term Memory) | **セル状態（Cell State）**を持つ<br>複雑なゲート構造で、長期的な<br>依存関係を制御する | 忘却ゲート<br>入力ゲート<br>3. **出力ゲート**<br><br>                | **メリット**：勾配消失を防ぎ、長期の<br>時系列データで高い性能を発揮。<br>**デメリット**：パラメータ数が多く、<br>計算コストが高い。        |
| GRU<br>(Gated Recurrent Unit)        | LSTMを簡略化したモデル<br>セル状態と隠れ状態を統合                          | **2つのゲート**：<br>1. **更新ゲート**<br>2. **リセットゲート**<br><br> | **メリット**：LSTMに比べ、パラメータが<br>少なく、高速に学習できる。<br>**デメリット**：タスクによってはLSTMより<br>精度が劣る場合がある。 |
