# RNN
隠れ層にリカレント結合を持ち、前の時刻の隠れ層の状態（情報）が、次の時刻の隠れ層の入力として渡される
→<font color="#ffff00">単語の並びや音声</font>、株価の推移といった時間的な文脈を持つデータを学習することができる
## RNNが直前の隠れ層の状態しか記憶できないという課題を克服

| 項目          | 特徴                                       | 図                                    |
| ----------- | ---------------------------------------- | ------------------------------------ |
| エルマンネットワーク  | 直前の隠れ状態を次の層の入力に<br><br>直前の文脈を考慮するタスク     | ![[Pasted image 20250901202322.png]] |
| ジョルダンネットワーク | 直前の最終的な出力を次の層の入力に<br><br>過去の行動履歴を考慮するタスク | ![[Pasted image 20250901202339.png]] |

## 勾配消失問題を克服

| モデル名                                 | 特徴                                   | 構造とゲート                          | メリット/デメリット                                                                    |
| ------------------------------------ | ------------------------------------ | ------------------------------- | ----------------------------------------------------------------------------- |
| **LSTM**<br>(Long Short-Term Memory) | セル状態()を持つ複雑なゲート構造で、<br>長期的な依存関係を制御する | 忘却ゲート<br>入力ゲート<br>出力ゲート<br><br> | メリット：<br>勾配消失を防ぎ、長期の時系列データで高い性能を発揮<br><br>デメリット：<br>パラメータ数が多く、計算コストが高い        |
| GRU<br>(Gated Recurrent Unit)        | LSTMを簡略化したモデル<br><br>セル状態と隠れ状態を統合    | 更新ゲート<br>リセットゲート<br><br>        | メリット：<br>LSTMに比べ、パラメータが少なく、高速に学習できる<br><br>デメリット：<br>タスクによってはLSTMより精度が劣る場合がある |
