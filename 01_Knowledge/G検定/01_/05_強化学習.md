強化学習における**割引率（Discount Factor）**は、将来得られる報酬を現在価値に換算するためのパラメータです。その値は0から1の間で設定され、**γ（ガンマ）**で表されます。

---

### 割引率の役割
強化学習の目的は、エージェントが**累積報酬**を最大化するように行動を学習すること
#### γが1に近い場合
将来の報酬をほとんど割り引かないため、エージェントは長期的な視点で行動を学習
#### γが0に近い場合
将来の報酬を大きく割り引くため、エージェントは直近の報酬を重視して行動を学習
### **バンディットアルゴリズム**
複数の選択肢の中から最適なものを効率的に見つけ出すための強化学習のアルゴリズム
探索（新しい選択肢を試すこと）と活用（最も良いとわかっている選択肢を選ぶこと）のバランスを最適化すること
![[Pasted image 20250901145630.png]]

---

### **ε-greedy方策**

**ε-greedy方策**は、バンディットアルゴリズムにおける探索と活用のバランスを取るためのシンプルな手法です。

- **活用（Exploitation）**: 通常は、これまでの試行で最も良い結果を出した選択肢（腕）を選びます。
    
- **探索（Exploration）**: しかし、一定の確率 **ε（イプシロン）** で、ランダムに他の選択肢を選びます。
    

この**ε**の値は、アルゴリズムの振る舞いを決定します。εを大きくすると探索が増え、新しい選択肢を発見しやすくなりますが、最適な選択肢を引く機会を逃します。逆に、εを小さくすると活用が増え、短期的な報酬は高まりますが、まだ見ぬ最適な選択肢を見落とす可能性があります。

---

### **マルコフ決定過程モデル（MDP）**

**マルコフ決定過程モデル（MDP）**は、強化学習の多くの問題を数学的に定式化するためのフレームワークです。MDPは、エージェントが環境と相互作用する過程をモデル化し、以下の要素で構成されます。

- **状態（State）**: エージェントが置かれている状況。
    
- **行動（Action）**: エージェントがその状態で行える選択肢。
    
- **遷移確率（Transition Probability）**: ある状態である行動をとったときに、次の状態へ移る確率。
    
- **報酬（Reward）**: ある状態である行動をとったときに得られる価値。
    
- **割引率（Discount Factor）**: 将来の報酬をどれだけ重視するかを示すパラメータ。
    

MDPは、これらの要素に基づいて、エージェントが**累積報酬を最大化する最適な方策**（各状態で行うべき行動の計画）をどう見つけるかを扱います。

---

### **マルコフ性（Markov Property）**

**マルコフ性**は、MDPの根幹をなす重要な概念です。 「**ある時点の未来の状態は、現在の状態にのみ依存し、それまでの過去の履歴には依存しない**」という性質を指します。

これは、エージェントが次にどのような状態に移るか、あるいはどのような報酬を得るかは、**現在の状態の情報だけ**で決まり、それ以前の行動や状態の履歴は関係ない、という前提を意味します。

この性質があることで、強化学習の計算をはるかに単純化でき、複雑な問題を解くことが可能になります。