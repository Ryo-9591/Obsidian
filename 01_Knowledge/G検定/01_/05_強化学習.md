強化学習における**割引率（Discount Factor）**は、将来得られる報酬を現在価値に換算するためのパラメータです。その値は0から1の間で設定され、**γ（ガンマ）**で表されます。

---
### 割引率の役割
強化学習の目的は、エージェントが**累積報酬**を最大化するように行動を学習すること
#### γが1に近い場合
将来の報酬をほとんど割り引かないため、エージェントは長期的な視点で行動を学習
#### γが0に近い場合
将来の報酬を大きく割り引くため、エージェントは直近の報酬を重視して行動を学習
### **バンディットアルゴリズム**
複数の選択肢の中から最適なものを効率的に見つけ出すための強化学習のアルゴリズム
探索（新しい選択肢を試すこと）と活用（最も良いとわかっている選択肢を選ぶこと）のバランスを最適化すること
![[Pasted image 20250901145630.png]]

---
### **ε-greedy方策**

**ε-greedy方策**は、バンディットアルゴリズムにおける探索と活用のバランスを取るためのシンプルな手法です。
- **活用（Exploitation）**: 通常は、これまでの試行で最も良い結果を出した選択肢（腕）を選びます。
- **探索（Exploration）**: しかし、一定の確率 **ε（イプシロン）** で、ランダムに他の選択肢を選びます。

この**ε**の値は、アルゴリズムの振る舞いを決定します。εを大きくすると探索が増え、新しい選択肢を発見しやすくなりますが、最適な選択肢を引く機会を逃します。逆に、εを小さくすると活用が増え、短期的な報酬は高まりますが、まだ見ぬ最適な選択肢を見落とす可能性があります。

---
### **マルコフ決定過程モデル（MDP）**
強化学習の多くの問題を数学的に定式化するためのフレームワークです。
エージェントが**累積報酬を最大化する最適な方策**（各状態で行うべき行動の計画）をどう見つけるかを扱います。

---
### **マルコフ性（Markov Property）**

**マルコフ性**は、MDPの根幹をなす重要な概念です。 「**ある時点の未来の状態は、現在の状態にのみ依存し、それまでの過去の履歴には依存しない**」という性質を指します。
### **価値関数（Value Function）**

価値関数は、ある特定の**状態**や、ある状態である**行動**をとったときに、将来にわたってどれだけの累積報酬が期待できるかを表す関数です。エージェントが「どの状態が良いか」「どの行動が良いか」を判断する基準となります。

価値関数には、主に2種類あります。

- **状態価値関数（V(s)）**: ある状態sにいるときに、将来得られる累積報酬の期待値。
    
- **行動価値関数（Q(s,a)）**: ある状態sで特定の行動aをとったときに、将来得られる累積報酬の期待値。
    

多くのアルゴリズム（例: Q学習）は、この価値関数を学習することで最適な行動を見つけ出します。