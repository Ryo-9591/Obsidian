強化学習における<font color="#ffff00">割引率</font>は、将来得られる報酬を現在価値に換算するためのパラメータ（0から1の間で設定される）
# バンディットアルゴリズム
複数の選択肢の中から最適なものを効率的に見つけ出すための強化学習のアルゴリズム
<font color="#ffff00">探索</font>（新しい選択肢を試すこと）と<font color="#ffff00">活用</font>（最も良いとわかっている選択肢を選ぶこと）のバランスを最適化すること

---
# **ε-greedy方策**
バンディットアルゴリズムにおける探索と活用のバランスを取るためのシンプルな手法

活用・・・これまでの試行で最も良い結果を出した選択肢を選ぶ
探索・・・一定の確率 ε（イプシロン） で、ランダムに他の選択肢を選ぶ

εを大きくすると探索が増え、新しい選択肢を発見しやすくなりますが、最適な選択肢を引く機会を逃します。
εを小さくすると活用が増え、短期的な報酬は高まりますが、まだ見ぬ最適な選択肢を見落とす可能性があります。

---
# **マルコフ決定過程モデル（MDP）**
強化学習の多くの問題を数学的に定式化するためのフレームワーク

エージェントが累積報酬を最大化する最適な方策（各状態で行うべき行動の計画）をどう見つけるかを扱う
## **マルコフ性（Markov Property）**
「<font color="#ffff00">ある時点の未来の状態は、現在の状態にのみ依存し、それまでの過去の履歴には依存しない</font>」という性質

---
# 価値関数（Value Function）（Q値）
ある特定の状態や、ある状態である行動をとったときに、将来にわたってどれだけの累積報酬が期待できるかを表す関数
## 状態価値関数（V(s)）
ある状態sにいるときに、将来得られる累積報酬の期待値。
## 行動価値関数（Q(s,a)） 
ある状態sで特定の行動aをとったときに、将来得られる累積報酬の期待値。

多くのアルゴリズム（例: <font color="#ffff00">Q学習</font>）は、この価値関数を学習することで最適な行動を見つける

---
# **方策勾配（Policy Gradient）**
方策勾配は、価値関数を直接学習するのではなく、最適な<font color="#ffff00">方策を直接学習する手法</font>です。
## 方策（Policy）
エージェントが特定の状態にいるときに、どの行動をどのくらいの確率でとるべきかを示す
##方策勾配法**

方策勾配法は、この方策関数を**勾配上昇法**を使って少しずつ更新し、累積報酬の期待値を最大化することを目指します。

1. **方策の実行**: 現在の方策に従って行動し、報酬を獲得します。
    
2. **勾配の計算**: 獲得した報酬が高いほど、その行動をとる確率を上げるように、方策を更新する方向（勾配）を計算します。
    
3. **方策の更新**: 計算された勾配に沿って方策を更新します。
    

この手法は、行動の選択肢が連続的で膨大な場合（例: ロボットの関節角度制御）に特に有効です。

### **価値関数と方策勾配の比較**

- **価値関数ベース**: 価値を計算し、その価値に基づいて行動を選択します。最適解を見つけるための探索が効率的ですが、行動の選択肢が多いと計算が複雑になります。
    
- **方策勾配ベース**: 価値を計算せずに、良い行動を直接学習します。行動の選択肢が多い問題でも柔軟に対応できますが、学習が不安定になることがあります。

### REINFORCE

**REINFORCE**は、方策勾配法（Policy Gradient）の最も基本的なアルゴリズムの一つです。この手法は、エージェントが特定の行動をとった結果、得られた報酬の総和（累積報酬）に基づいて、その行動を**強化**（reinforce）します。

- **仕組み**: エージェントはまず、現在の方策（行動方針）に従って行動を試行し、一連の行動と報酬の記録を生成します。その上で、最終的に得られた累積報酬が大きければ、その一連の行動をとる確率を上げ、小さければ確率を下げるように方策を更新します。
    
- **特徴**: シンプルで直感的ですが、報酬がエピソードの終わりにしか得られない場合、どの行動が本当に良かったのかを判断するのが難しく、学習が不安定になることがあります。
    

---

### Actor-Critic

**Actor-Critic**は、REINFORCEの不安定性を改善するために考案された方策勾配法です。名前が示す通り、2つの主要なコンポーネントで構成されています。

1. **Actor（アクター）**: 俳優のように**行動を選択**する役割を担います。方策関数を学習し、どの状態ではどの行動をとるべきかを決定します。
    
2. **Critic（クリティック）**: 批評家のように**行動を評価**する役割を担います。価値関数を学習し、アクターがとった行動がどれだけ良かったかを評価します。
    

- **仕組み**: アクターがある行動をとると、クリティックがその行動の価値を評価し、その評価をアクターにフィードバックします。アクターは、クリティックからの評価（**ベースライン**）に基づいて方策を更新します。これにより、エージェントはエピソードの終わりを待つことなく、より効率的に学習を進めることができます。