# ニューラルネットワーク
## 単純パーセプトロン
複数の特徴量を受け取り1つの出力を出すNN
![[Pasted image 20250901160336.png]]
## 多層パーセプトロン
![[Pasted image 20250901160519.png]]
# ディープラーニング
ニューラルネットワークの隠れ層を増やしたもの
## パラメータ最適化手法

| 勾配下降法               | 説明                                                                                                         |
| ------------------- | ---------------------------------------------------------------------------------------------------------- |
| バッチ下降法（最急下降法、バッチ学習） | 全データ（<font color="#ffff00">エポック</font>）の予測誤差の総和を用いて更新式の計算<br><font color="#ffff00">局所最適解</font>に落ち着く可能性がある |
| 確率的勾配下降法（オンライン学習）   | <font color="#ffff00">ランダムに抽出して</font>更新式を計算<br><font color="#ffff00">大域最適解</font>を見つけられるかも                |
| ミニバッチ勾配下降法          | 全データを小さいデータセット（<font color="#ffff00">イテレーション</font>）にしてそれごとに更新式を計算                                         |
### 勾配下降法の問題
<font color="#ffff00">鞍点</font>に陥る可能性→こうした停滞状態を<font color="#ffff00">プラトー</font>
![[Pasted image 20250901162518.png]]
### 鞍点への対策

| 手法                | 説明                                                                                           |
| ----------------- | -------------------------------------------------------------------------------------------- |
| モーメンタム            | 最適化方向へ学習を加速させる方法                                                                             |
| Adagrad           | 頻繁に出現する特徴量には小さな学習率を、珍しい特徴量には大きな学習率を適用するが、学習が進むと<font color="#ffff00">学習率がゼロに収束してしまう欠点</font> |
| Adadelta          | AdaGradの欠点を改善した手法。過去の勾配の蓄積を抑えることで、学習率が極端に小さくなるのを防ぎ、学習率のハイパーパラメータを不要に                         |
| RMSprop           | AdaGradの問題を改善した手法で、勾配の二乗の移動平均を学習率の調整に用います。学習が安定し、収束が速い                                       |
| Adam              | RMSpropのアイデアに加えて、過去の勾配の慣性（勢い）も考慮に入れることで、高速かつ安定した学習                                           |
| AdaBound・AMSBound | Adamの学習率が発散する可能性を改善した手法。学習率に上限と下限を設けることで、学習の初期はAdamのように速く、終盤はSGDのように安定して収束する                 |
## 早期終了
誤差関数が右肩上がりになったときに学習を打ち切る
<font color="#ffff00">ノーフリーランチ定理</font>（<font color="#ffff00">あらゆる問題で性能の良い汎用最適化戦力は理論所不可能である</font>）を意識
ただ、<font color="#ffff00">二重降下現象</font>（1回誤差増えた後に下がる現象）もあるので学習を止めるタイミングは検討が必要
## ハイパーパラメータの最適化

| ハイパーパラメータチューニング | 説明                                                                                  |
| --------------- | ----------------------------------------------------------------------------------- |
| グリッドサーチ         | すべてのハイパーパラメータの組み合わせを、網羅的に試す手法。最も単純で確実だが、計算コストが非常に高い                                 |
| ランダムサーチ         | ランダムに選んだハイパーパラメータの組み合わせを試す手法。グリッドサーチよりも効率的に良い組み合わせを見つけられることが多い                      |
| ベイズ最適化          | これまでの試行結果から、次に試すべき最適なハイパーパラメータの組み合わせを予測し、効率的に探索する手法。計算コストは高いが、より少ない試行回数で最適な解を見つけやすい |
| 遺伝子アルゴリズム       | 生物の進化を模倣した探索手法。複数の候補（遺伝子）をランダムに生成し、評価の高い候補を組み合わせて、より良い世代（組み合わせ）を作り出す。               |
## 誤差逆伝搬
勾配降下法を用いてニューラルネットワークの学習をする際に微分計算を



## 活性化関数
層の間をどのように伝搬させるか調整する関数
### 



