# ニューラルネットワーク
## 単純パーセプトロン
複数の特徴涼を受け取り1つの出力を出すNN
![[Pasted image 20250901160336.png]]
## 多層パーセプトロン
![[Pasted image 20250901160519.png]]
# ディープラーニング
ニューラルネットワークの隠れ層を増やしたもの
## パラメータ最適化手法
### 勾配下降法
#### バッチ下降法（最急下降法、バッチ学習）
全データ（<font color="#ffff00">エポック</font>）の予測誤差の総和を用いて更新式の計算
<font color="#ffff00">局所最適解</font>に落ち着く可能性がある
#### 確率的勾配下降法（オンライン学習）
<font color="#ffff00">ランダムに抽出して</font>更新式を計算
<font color="#ffff00">大域最適解</font>を見つけられるかも
#### ミニバッチ勾配下降法
全データを小さいデータセット（<font color="#ffff00">イテレーション</font>）にしてそれごとに更新式を計算

### 勾配下降法の問題
<font color="#ffff00">鞍点</font>に陥る可能性→こうした停滞状態を<font color="#ffff00">プラトー</font>
![[Pasted image 20250901162518.png]]
### 鞍点への対策

| 手法                | 説明                                                                                           |
| ----------------- | -------------------------------------------------------------------------------------------- |
| モーメンタム            | 最適化方向へ学習を加速させる方法                                                                             |
| Adagrad           | 頻繁に出現する特徴量には小さな学習率を、珍しい特徴量には大きな学習率を適用するが、学習が進むと<font color="#ffff00">学習率がゼロに収束してしまう欠点</font> |
| Adadelta          | AdaGradの欠点を改善した手法。過去の勾配の蓄積を抑えることで、学習率が極端に小さくなるのを防ぎ、学習率のハイパーパラメータを不要に                         |
| RMSprop           | AdaGradの問題を改善した手法で、勾配の二乗の移動平均を学習率の調整に用います。学習が安定し、収束が速い                                       |
| Adam              | RMSpropのアイデアに加えて、過去の勾配の慣性（勢い）も考慮に入れることで、高速かつ安定した学習                                           |
| AdaBound・AMSBound | Adamの学習率が発散する可能性を改善した手法。学習率に上限と下限を設けることで、学習の初期はAdamのように速く、終盤はSGDのように安定して収束する                 |
## 早期終了
誤差関数が右肩上がりになったときに学習を打ち切る
<font color="#ffff00">ノーフリーランチ定理</font>（<font color="#ffff00">あらゆる問題で性能の良い汎用最適化戦力は理論所不可能である</font>）を意識
ただ、<font color="#ffff00">二重降下現象</font>（1回誤差増えた後に下がる現象）もあるので学習を止めるタイミングは検討が必要

## 誤差逆伝搬

## 活性化関数
層の間をどのように伝搬させるか調整する関数
### 



